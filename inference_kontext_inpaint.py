#!/usr/bin/env python3
"""
Kontext-inpaint ä¼ª mask-free æ¨ç†è„šæœ¬
å®ç°åŸå›¾ + çº¯ç™½æ§åˆ¶å›¾ â†’ ç¼–è¾‘ç»“æœçš„æ¨ç†æµç¨‹
æ”¯æŒå¤šè½®ç¼–è¾‘å’Œä¸€è‡´æ€§ä¿æŒ
"""

import torch
import argparse
from PIL import Image
import os
from diffusers import FluxFillPipeline
from pathlib import Path
import numpy as np


class KontextInpaintInference:
    """Kontext-inpaint æ¨ç†å™¨"""
    
    def __init__(self, model_path, device="cuda", dtype=torch.bfloat16):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            dtype: æ•°æ®ç±»å‹
        """
        self.device = device
        self.dtype = dtype
        self.model_path = model_path
        
        print(f"ğŸš€ åŠ è½½ Kontext-inpaint æ¨¡å‹: {model_path}")
        self.pipeline = self._load_pipeline()
        
    def _load_pipeline(self):
        """åŠ è½½ FluxFillPipelineï¼ˆåŸºäº FLUX.1-Fillï¼‰"""
        try:
            pipeline = FluxFillPipeline.from_pretrained(
                self.model_path,
                torch_dtype=self.dtype,
                device_map=self.device
            )
            
            # å†…å­˜ä¼˜åŒ–
            try:
                pipeline.enable_model_cpu_offload()
                print("âœ… å¯ç”¨ CPU offload")
            except:
                print("âš ï¸ CPU offload ä¸å¯ç”¨")
                
            try:
                pipeline.enable_attention_slicing()
                print("âœ… å¯ç”¨ attention slicing")
            except:
                print("âš ï¸ Attention slicing ä¸å¯ç”¨")
                
            return pipeline
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def create_white_control_image(self, source_image):
        """
        åˆ›å»ºçº¯ç™½æ§åˆ¶å›¾åƒ
        
        Args:
            source_image: PIL.Imageï¼ŒåŸå§‹å›¾åƒ
            
        Returns:
            PIL.Image: çº¯ç™½æ§åˆ¶å›¾åƒ (255,255,255)
        """
        width, height = source_image.size
        white_image = Image.new('RGB', (width, height), (255, 255, 255))
        return white_image
    
    def prepare_images(self, source_path, target_size=(512, 512)):
        """
        å‡†å¤‡è¾“å…¥å›¾åƒï¼šåŸå›¾ + çº¯ç™½æ§åˆ¶å›¾
        
        Args:
            source_path: åŸå›¾è·¯å¾„
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            tuple: (source_image, white_control_image)
        """
        # åŠ è½½åŸå›¾
        source_image = Image.open(source_path)
        if source_image.mode != 'RGB':
            source_image = source_image.convert('RGB')
        
        # åˆ›å»ºçº¯ç™½æ§åˆ¶å›¾
        white_control_image = self.create_white_control_image(source_image)
        
        # è°ƒæ•´å°ºå¯¸
        source_image = source_image.resize(target_size, Image.LANCZOS)
        white_control_image = white_control_image.resize(target_size, Image.LANCZOS)
        
        return source_image, white_control_image
    
    def run_inference(self, source_image, white_control_image, prompt, 
                     num_inference_steps=20, guidance_scale=4.0, seed=None):
        """
        è¿è¡Œ Kontext-inpaint æ¨ç†
        
        Args:
            source_image: PIL.Imageï¼ŒåŸå§‹å›¾åƒ
            white_control_image: PIL.Imageï¼Œçº¯ç™½æ§åˆ¶å›¾åƒ
            prompt: strï¼Œç¼–è¾‘æç¤ºè¯
            num_inference_steps: intï¼Œæ¨ç†æ­¥æ•°
            guidance_scale: floatï¼Œå¼•å¯¼å¼ºåº¦
            seed: intï¼Œéšæœºç§å­
            
        Returns:
            PIL.Image: ç¼–è¾‘åçš„å›¾åƒ
        """
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
            print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {seed}")
        
        print(f"ğŸ¨ å¼€å§‹ Kontext-inpaint æ¨ç†...")
        print(f"   ğŸ“ æç¤ºè¯: {prompt}")
        print(f"   ğŸ”„ æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"   ğŸ¯ å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        print(f"   ğŸ­ æ§åˆ¶æ¨¡å¼: çº¯ç™½æ§åˆ¶å›¾ (ä¼ª mask-free)")
        
        with torch.no_grad():
            # ä½¿ç”¨ FluxFillPipeline è¿›è¡Œæ¨ç†
            # åŸºäº FLUX.1-Fillï¼Œä½¿ç”¨çº¯ç™½å›¾ä½œä¸º mask
            result = self.pipeline(
                prompt=prompt,
                image=source_image,  # åŸå›¾ä½œä¸ºè¾“å…¥
                mask_image=white_control_image,  # çº¯ç™½æ§åˆ¶å›¾ä½œä¸ºmask
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
                height=source_image.height,
                width=source_image.width,
            )
        
        return result.images[0]
    
    def multi_round_edit(self, source_path, edit_prompts, output_dir, 
                        num_inference_steps=20, guidance_scale=4.0, seed=42):
        """
        å¤šè½®ç¼–è¾‘åŠŸèƒ½ï¼šä¿æŒä¸€è‡´æ€§çš„è¿ç»­ç¼–è¾‘
        
        Args:
            source_path: åŸå›¾è·¯å¾„
            edit_prompts: listï¼Œç¼–è¾‘æç¤ºè¯åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦  
            seed: éšæœºç§å­
        """
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ”„ å¼€å§‹å¤šè½®ç¼–è¾‘ï¼Œå…± {len(edit_prompts)} è½®")
        
        # ç¬¬ä¸€è½®ï¼šä»åŸå›¾å¼€å§‹
        current_image_path = source_path
        
        for i, prompt in enumerate(edit_prompts):
            print(f"\nğŸ¯ ç¬¬ {i+1} è½®ç¼–è¾‘: {prompt}")
            
            # å‡†å¤‡å›¾åƒ
            source_image, white_control_image = self.prepare_images(current_image_path)
            
            # æ‰§è¡Œæ¨ç†
            result_image = self.run_inference(
                source_image=source_image,
                white_control_image=white_control_image,
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed + i  # æ¯è½®ä½¿ç”¨ä¸åŒç§å­
            )
            
            # ä¿å­˜ç»“æœ
            output_path = os.path.join(output_dir, f"round_{i+1:02d}_{prompt[:30].replace(' ', '_')}.png")
            result_image.save(output_path)
            print(f"âœ… ç¬¬ {i+1} è½®å®Œæˆï¼Œä¿å­˜åˆ°: {output_path}")
            
            # æ›´æ–°å½“å‰å›¾åƒä¸ºä¸‹ä¸€è½®çš„è¾“å…¥
            current_image_path = output_path
        
        print(f"\nğŸ‰ å¤šè½®ç¼–è¾‘å®Œæˆï¼æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Kontext-inpaint ä¼ª mask-free æ¨ç†")
    parser.add_argument("--model_path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--source_image", required=True, help="æºå›¾åƒè·¯å¾„")
    parser.add_argument("--prompt", required=True, help="ç¼–è¾‘æç¤ºè¯")
    parser.add_argument("--output", default="kontext_inpaint_result.png", help="è¾“å‡ºå›¾åƒè·¯å¾„")
    parser.add_argument("--steps", type=int, default=20, help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--guidance", type=float, default=4.0, help="å¼•å¯¼å¼ºåº¦")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--size", type=int, default=512, help="å›¾åƒå°ºå¯¸")
    
    # å¤šè½®ç¼–è¾‘é€‰é¡¹
    parser.add_argument("--multi_round", action="store_true", help="å¯ç”¨å¤šè½®ç¼–è¾‘æ¨¡å¼")
    parser.add_argument("--prompts_file", help="å¤šè½®ç¼–è¾‘æç¤ºè¯æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    parser.add_argument("--output_dir", default="multi_round_results", help="å¤šè½®ç¼–è¾‘è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.source_image):
        print(f"âŒ æºå›¾åƒä¸å­˜åœ¨: {args.source_image}")
        return
    
    print("ğŸ­ Kontext-inpaint ä¼ª mask-free æ¨ç†")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"ğŸ–¼ï¸ æºå›¾åƒ: {args.source_image}")
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    try:
        inferencer = KontextInpaintInference(args.model_path)
    except Exception as e:
        print(f"âŒ æ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    if args.multi_round and args.prompts_file:
        # å¤šè½®ç¼–è¾‘æ¨¡å¼
        print(f"ğŸ”„ å¤šè½®ç¼–è¾‘æ¨¡å¼")
        print(f"ğŸ“ æç¤ºè¯æ–‡ä»¶: {args.prompts_file}")
        
        if not os.path.exists(args.prompts_file):
            print(f"âŒ æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {args.prompts_file}")
            return
            
        # è¯»å–æç¤ºè¯
        with open(args.prompts_file, 'r', encoding='utf-8') as f:
            edit_prompts = [line.strip() for line in f if line.strip()]
        
        # æ‰§è¡Œå¤šè½®ç¼–è¾‘
        inferencer.multi_round_edit(
            source_path=args.source_image,
            edit_prompts=edit_prompts,
            output_dir=args.output_dir,
            num_inference_steps=args.steps,
            guidance_scale=args.guidance,
            seed=args.seed
        )
        
    else:
        # å•æ¬¡æ¨ç†æ¨¡å¼
        print(f"ğŸ’¬ ç¼–è¾‘æç¤º: {args.prompt}")
        
        # å‡†å¤‡å›¾åƒ
        try:
            source_image, white_control_image = inferencer.prepare_images(
                args.source_image, 
                target_size=(args.size, args.size)
            )
            print(f"âœ… å›¾åƒé¢„å¤„ç†å®Œæˆ: {args.size}x{args.size}")
        except Exception as e:
            print(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            return
        
        # è¿è¡Œæ¨ç†
        try:
            result_image = inferencer.run_inference(
                source_image=source_image,
                white_control_image=white_control_image,
                prompt=args.prompt,
                num_inference_steps=args.steps,
                guidance_scale=args.guidance,
                seed=args.seed
            )
            
            # ä¿å­˜ç»“æœ
            result_image.save(args.output)
            print(f"âœ… æ¨ç†å®Œæˆï¼ç»“æœä¿å­˜åˆ°: {args.output}")
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()