#!/usr/bin/env python3
"""
Kontext-inpaint ä¼ª mask-free æ¨ç†è„šæœ¬
å®ç°åŸå›¾ + çº¯ç™½æ§åˆ¶å›¾ â†’ ç¼–è¾‘ç»“æœçš„æ¨ç†æµç¨‹
æ”¯æŒå¤šè½®ç¼–è¾‘å’Œä¸€è‡´æ€§ä¿æŒ
"""

import torch
import argparse
from PIL import Image
import os
from diffusers import FluxFillPipeline, FluxTransformer2DModel, AutoencoderKL
from transformers import CLIPTextModelWithProjection, T5EncoderModel, CLIPTokenizer, T5TokenizerFast
from pathlib import Path
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as tv_transforms


class KontextInpaintInference:
    """Kontext-inpaint æ¨ç†å™¨"""
    
    def __init__(self, model_path, device="cuda", dtype=torch.bfloat16, base_model_path: str | None = None):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            dtype: æ•°æ®ç±»å‹
        """
        self.device = device
        self.dtype = dtype
        self.model_path = model_path
        self.base_model_path = base_model_path
        self.ckpt_mode = False
        
        print(f"ğŸš€ åŠ è½½ Kontext-inpaint æ¨¡å‹: {model_path}")
        self.pipeline = self._load_pipeline()
        
    def _load_pipeline(self):
        """åŠ è½½ FluxFillPipelineï¼ˆåŸºäº FLUX.1-Fillï¼‰"""
        try:
            model_index_path = os.path.join(self.model_path, "model_index.json")
            if os.path.exists(model_index_path):
                # æ ‡å‡† Diffusers ç®¡çº¿ç›®å½•
                pipeline = FluxFillPipeline.from_pretrained(
                    self.model_path,
                    torch_dtype=self.dtype
                )
            else:
                # å…¼å®¹è®­ç»ƒä¿å­˜çš„ checkpoint ç›®å½•ï¼ˆä»…åŒ…å«ç»„ä»¶å­æ–‡ä»¶å¤¹ï¼‰
                # éœ€è¦æä¾› base_model_path ä½œä¸ºç®¡çº¿æ¨¡æ¿
                if not self.base_model_path or not os.path.isdir(self.base_model_path):
                    raise ValueError(
                        f"ç¼ºå°‘ base_model_pathï¼ˆæˆ–æ— æ•ˆï¼‰ã€‚checkpoint ç›®å½•ä¸å« model_index.jsonï¼Œéœ€æä¾›åŸºç¡€æ¨¡å‹ç›®å½•ä½œä¸ºæ¨¡æ¿ã€‚ä¾‹å¦‚: --base_model_path /cloud/cloud-ssd1/FLUX.1-Fill-dev"
                    )
                print(f"ğŸ”§ ä½¿ç”¨åŸºç¡€æ¨¡å‹æ¨¡æ¿æ„å»ºç®¡çº¿: {self.base_model_path}")
                pipeline = FluxFillPipeline.from_pretrained(self.base_model_path, torch_dtype=self.dtype)

                # é€é¡¹æ›¿æ¢ä¸º checkpoint ä¸­çš„ç»„ä»¶
                ckpt = self.model_path
                if os.path.isdir(os.path.join(ckpt, "transformer")):
                    transformer = FluxTransformer2DModel.from_pretrained(os.path.join(ckpt, "transformer"), torch_dtype=self.dtype)
                    pipeline.transformer = transformer
                if os.path.isdir(os.path.join(ckpt, "vae")):
                    vae = AutoencoderKL.from_pretrained(os.path.join(ckpt, "vae"), torch_dtype=self.dtype)
                    pipeline.vae = vae
                if os.path.isdir(os.path.join(ckpt, "text_encoder")):
                    te = CLIPTextModelWithProjection.from_pretrained(os.path.join(ckpt, "text_encoder"), torch_dtype=self.dtype)
                    pipeline.text_encoder = te
                if os.path.isdir(os.path.join(ckpt, "text_encoder_2")):
                    te2 = T5EncoderModel.from_pretrained(os.path.join(ckpt, "text_encoder_2"), torch_dtype=self.dtype)
                    pipeline.text_encoder_2 = te2
                if os.path.isdir(os.path.join(ckpt, "tokenizer")):
                    pipeline.tokenizer = CLIPTokenizer.from_pretrained(os.path.join(ckpt, "tokenizer"))
                if os.path.isdir(os.path.join(ckpt, "tokenizer_2")):
                    pipeline.tokenizer_2 = T5TokenizerFast.from_pretrained(os.path.join(ckpt, "tokenizer_2"))

                # å¯ç”¨ checkpoint å…¼å®¹æ¨¡å¼ï¼ˆè‡ªå®šä¹‰å‰å‘ï¼Œç»•è¿‡ pipeline.__call__ï¼‰
                self.ckpt_mode = True

            # å°†æ•´ä¸ª pipeline ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            pipeline = pipeline.to(self.device)
            
            # å†…å­˜ä¼˜åŒ–
            try:
                pipeline.enable_model_cpu_offload()
                print("âœ… å¯ç”¨ CPU offload")
            except:
                print("âš ï¸ CPU offload ä¸å¯ç”¨")
                
            try:
                pipeline.enable_attention_slicing()
                print("âœ… å¯ç”¨ attention slicing")
            except:
                print("âš ï¸ Attention slicing ä¸å¯ç”¨")
                
            return pipeline
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

    def _encode_prompt_clip(self, prompt: str):
        # ä½¿ç”¨ CLIP ç¼–ç ï¼Œå¹¶åŠ è½½æŠ•å½±å±‚æƒé‡ï¼ˆæ¥è‡ªè®­ç»ƒä¿å­˜çš„ proj_hidden.pt / proj_pooled.ptï¼‰
        assert hasattr(self, 'pipeline'), "pipeline æœªåˆå§‹åŒ–"
        pipe = self.pipeline
        device = self.device
        dtype = self.dtype
        # æ–‡æœ¬ç¼–ç ï¼ˆCPUä¹Ÿå¯ï¼Œè¿™é‡Œç›´æ¥åœ¨GPUä¸Šï¼‰
        clip_ids = pipe.tokenizer(
            prompt,
            padding='max_length',
            max_length=pipe.tokenizer.model_max_length,
            truncation=True,
            return_tensors='pt'
        ).input_ids.to(device)
        clip_out = pipe.text_encoder(clip_ids, output_hidden_states=True, return_dict=True)
        hidden_states = clip_out.last_hidden_state  # [1, 77, hidden]
        pooled_state = clip_out.text_embeds  # [1, 768]

        # åŠ è½½æŠ•å½±å±‚ï¼ˆæ¥è‡ªcheckpointç›®å½•ï¼‰
        proj_hidden_path = os.path.join(self.model_path, 'proj_hidden.pt')
        proj_pooled_path = os.path.join(self.model_path, 'proj_pooled.pt')
        if not (os.path.exists(proj_hidden_path) and os.path.exists(proj_pooled_path)):
            raise FileNotFoundError("æœªæ‰¾åˆ° proj_hidden.pt æˆ– proj_pooled.ptï¼Œè¯·ç¡®è®¤ checkpoint ç›®å½•åŒ…å«è¿™ä¸¤ä¸ªæ–‡ä»¶")
        # ä¾æ® transformer é…ç½®åˆ›å»ºæŠ•å½±å±‚å¹¶åŠ è½½
        joint_dim = int(pipe.transformer.config.joint_attention_dim)
        proj_hidden = nn.Linear(pipe.text_encoder.config.hidden_size, joint_dim).to(device, dtype=dtype)
        pooled_dim = pipe.transformer.config.get('pooled_projection_dim', 768)
        proj_pooled = nn.Linear(pipe.text_encoder.config.projection_dim, pooled_dim).to(device, dtype=dtype)
        proj_hidden.load_state_dict(torch.load(proj_hidden_path, map_location=device))
        proj_pooled.load_state_dict(torch.load(proj_pooled_path, map_location=device))
        proj_hidden.eval(); proj_pooled.eval()

        with torch.no_grad():
            enc_states = proj_hidden(hidden_states.to(device, dtype=dtype))
            pool_proj = proj_pooled(pooled_state.to(device, dtype=dtype))
            # ç¨³å¥åŒ–
            enc_states = torch.clamp(enc_states, -5.0, 5.0)
            pool_proj = torch.clamp(pool_proj, -5.0, 5.0)
        return enc_states, pool_proj

    def _run_inference_ckpt(self, source_image: Image.Image, white_control_image: Image.Image, prompt: str,
                             num_inference_steps: int, guidance_scale: float, generator: torch.Generator | None):
        pipe = self.pipeline
        device = self.device
        dtype = self.dtype

        # 1) å›¾åƒé¢„å¤„ç†åˆ° [-1,1]
        transform = tv_transforms.Compose([
            tv_transforms.ToTensor(),
            # normalize to [-1,1]
            tv_transforms.Lambda(lambda x: x * 2 - 1)
        ])
        source_tensor = transform(source_image).unsqueeze(0).to(device, dtype=dtype)
        control_tensor = transform(white_control_image).unsqueeze(0).to(device, dtype=dtype)

        # 2) VAE ç¼–ç ä¸º latent
        with torch.no_grad():
            lat_source = pipe.vae.encode(source_tensor).latent_dist.sample()
            lat_control = pipe.vae.encode(control_tensor).latent_dist.sample()

        # 3) å–å‰16é€šé“ï¼Œæ‹¼æ¥å¹¶è¡¥é›¶åˆ°64
        lat_source = lat_source[:, :16]
        lat_control = lat_control[:, :16]
        model_input = torch.cat([lat_source, lat_control], dim=1)  # [B,32,H,W]
        if model_input.shape[1] < 64:
            pad = torch.zeros(model_input.shape[0], 64 - model_input.shape[1], model_input.shape[2], model_input.shape[3],
                              device=device, dtype=dtype)
            model_input = torch.cat([model_input, pad], dim=1)

        # 4) patchify (patch_size=1) â†’ [B*H*W, 64]
        B, C, H, W = model_input.shape
        patches = model_input.unfold(2, 1, 1).unfold(3, 1, 1)
        patches = patches.contiguous().view(B, C, -1, 1, 1)
        patches = patches.permute(0, 2, 1, 3, 4).reshape(-1, C * 1 * 1)

        # 5) æ–‡æœ¬ç¼–ç  + æŠ•å½±
        enc_states, pool_proj = self._encode_prompt_clip(prompt)

        # 6) ä½ç½® ID + timestep/guidance
        txt_len = enc_states.shape[1]
        # 2D idsï¼ˆå»æ‰batchç»´ï¼Œç¬¦åˆdiffusersæ–°æ¥å£è¦æ±‚ï¼‰
        txt_ids = torch.zeros((txt_len, 2), device=device, dtype=torch.long)
        y_coords = torch.arange(H, device=device).unsqueeze(1).repeat(1, W).flatten()
        x_coords = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1).flatten()
        img_ids_single = torch.stack([y_coords, x_coords], dim=1)
        img_ids = img_ids_single  # 2D
        timesteps = torch.zeros(B, dtype=torch.float32, device=device)
        guidance = timesteps.clone()

        # 7) Transformer å‰å‘
        with torch.autocast(device_type='cuda', dtype=dtype):
            pred = pipe.transformer(
                hidden_states=patches,
                encoder_hidden_states=enc_states,
                pooled_projections=pool_proj,
                guidance=guidance,
                timestep=timesteps,
                txt_ids=txt_ids,
                img_ids=img_ids,
                return_dict=False
            )[0]

        # 8) è¿˜åŸå›¾åƒ latent å¹¶è§£ç 
        img_seq_len = pred.shape[1]
        h = w = int(img_seq_len ** 0.5)
        img_pred_reshaped = pred.permute(0, 2, 1).reshape(B, pred.shape[2], h, w)
        img_pred_matched = img_pred_reshaped[:, :16]
        with torch.no_grad():
            decoded = pipe.vae.decode(img_pred_matched).sample
        decoded = (decoded / 2 + 0.5).clamp(0, 1)
        decoded = decoded.to(torch.float32)
        img = decoded[0].detach().cpu().permute(1, 2, 0).numpy()
        img = (img * 255).astype(np.uint8)
        return Image.fromarray(img)
    
    def create_white_control_image(self, source_image):
        """
        åˆ›å»ºçº¯ç™½æ§åˆ¶å›¾åƒ
        
        Args:
            source_image: PIL.Imageï¼ŒåŸå§‹å›¾åƒ
            
        Returns:
            PIL.Image: çº¯ç™½æ§åˆ¶å›¾åƒ (255,255,255)
        """
        width, height = source_image.size
        white_image = Image.new('RGB', (width, height), (255, 255, 255))
        return white_image
    
    def prepare_images(self, source_path, target_size=(512, 512)):
        """
        å‡†å¤‡è¾“å…¥å›¾åƒï¼šåŸå›¾ + çº¯ç™½æ§åˆ¶å›¾
        
        Args:
            source_path: åŸå›¾è·¯å¾„
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            tuple: (source_image, white_control_image)
        """
        # åŠ è½½åŸå›¾
        source_image = Image.open(source_path)
        if source_image.mode != 'RGB':
            source_image = source_image.convert('RGB')
        
        # åˆ›å»ºçº¯ç™½æ§åˆ¶å›¾
        white_control_image = self.create_white_control_image(source_image)
        
        # è°ƒæ•´å°ºå¯¸
        source_image = source_image.resize(target_size, Image.LANCZOS)
        white_control_image = white_control_image.resize(target_size, Image.LANCZOS)
        
        return source_image, white_control_image
    
    def run_inference(self, source_image, white_control_image, prompt, 
                     num_inference_steps=20, guidance_scale=4.0, seed=None):
        """
        è¿è¡Œ Kontext-inpaint æ¨ç†
        
        Args:
            source_image: PIL.Imageï¼ŒåŸå§‹å›¾åƒ
            white_control_image: PIL.Imageï¼Œçº¯ç™½æ§åˆ¶å›¾åƒ
            prompt: strï¼Œç¼–è¾‘æç¤ºè¯
            num_inference_steps: intï¼Œæ¨ç†æ­¥æ•°
            guidance_scale: floatï¼Œå¼•å¯¼å¼ºåº¦
            seed: intï¼Œéšæœºç§å­
            
        Returns:
            PIL.Image: ç¼–è¾‘åçš„å›¾åƒ
        """
        generator = None
        if seed is not None:
            generator = torch.Generator(device=self.device).manual_seed(seed)
            print(f"ğŸ² ä½¿ç”¨éšæœºç§å­: {seed}")
        
        print(f"ğŸ¨ å¼€å§‹ Kontext-inpaint æ¨ç†...")
        print(f"   ğŸ“ æç¤ºè¯: {prompt}")
        print(f"   ğŸ”„ æ¨ç†æ­¥æ•°: {num_inference_steps}")
        print(f"   ğŸ¯ å¼•å¯¼å¼ºåº¦: {guidance_scale}")
        print(f"   ğŸ­ æ§åˆ¶æ¨¡å¼: çº¯ç™½æ§åˆ¶å›¾ (ä¼ª mask-free)")
        
        if self.ckpt_mode:
            return self._run_inference_ckpt(
                source_image, white_control_image, prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator
            )
        else:
            with torch.no_grad():
                result = self.pipeline(
                    prompt=prompt,
                    image=source_image,
                    mask_image=white_control_image,
                    num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    height=source_image.height,
                    width=source_image.width,
                )
            return result.images[0]
    
    def multi_round_edit(self, source_path, edit_prompts, output_dir, 
                        num_inference_steps=20, guidance_scale=4.0, seed=42):
        """
        å¤šè½®ç¼–è¾‘åŠŸèƒ½ï¼šä¿æŒä¸€è‡´æ€§çš„è¿ç»­ç¼–è¾‘
        
        Args:
            source_path: åŸå›¾è·¯å¾„
            edit_prompts: listï¼Œç¼–è¾‘æç¤ºè¯åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            num_inference_steps: æ¨ç†æ­¥æ•°
            guidance_scale: å¼•å¯¼å¼ºåº¦  
            seed: éšæœºç§å­
        """
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ”„ å¼€å§‹å¤šè½®ç¼–è¾‘ï¼Œå…± {len(edit_prompts)} è½®")
        
        # ç¬¬ä¸€è½®ï¼šä»åŸå›¾å¼€å§‹
        current_image_path = source_path
        
        for i, prompt in enumerate(edit_prompts):
            print(f"\nğŸ¯ ç¬¬ {i+1} è½®ç¼–è¾‘: {prompt}")
            
            # å‡†å¤‡å›¾åƒ
            source_image, white_control_image = self.prepare_images(current_image_path)
            
            # æ‰§è¡Œæ¨ç†
            result_image = self.run_inference(
                source_image=source_image,
                white_control_image=white_control_image,
                prompt=prompt,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                seed=seed + i  # æ¯è½®ä½¿ç”¨ä¸åŒç§å­
            )
            
            # ä¿å­˜ç»“æœ
            output_path = os.path.join(output_dir, f"round_{i+1:02d}_{prompt[:30].replace(' ', '_')}.png")
            result_image.save(output_path)
            print(f"âœ… ç¬¬ {i+1} è½®å®Œæˆï¼Œä¿å­˜åˆ°: {output_path}")
            
            # æ›´æ–°å½“å‰å›¾åƒä¸ºä¸‹ä¸€è½®çš„è¾“å…¥
            current_image_path = output_path
        
        print(f"\nğŸ‰ å¤šè½®ç¼–è¾‘å®Œæˆï¼æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Kontext-inpaint ä¼ª mask-free æ¨ç†")
    parser.add_argument("--model_path", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base_model_path", required=False, help="å½“ model_path ä¸º checkpoint ç›®å½•æ—¶æä¾›åŸºç¡€æ¨¡å‹ç›®å½•ï¼ˆä¾‹å¦‚ FLUX.1-Fill-devï¼‰")
    parser.add_argument("--source_image", required=True, help="æºå›¾åƒè·¯å¾„")
    parser.add_argument("--prompt", required=True, help="ç¼–è¾‘æç¤ºè¯")
    parser.add_argument("--output", default="kontext_inpaint_result.png", help="è¾“å‡ºå›¾åƒè·¯å¾„")
    parser.add_argument("--steps", type=int, default=20, help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--guidance", type=float, default=4.0, help="å¼•å¯¼å¼ºåº¦")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--size", type=int, default=512, help="å›¾åƒå°ºå¯¸")
    
    # å¤šè½®ç¼–è¾‘é€‰é¡¹
    parser.add_argument("--multi_round", action="store_true", help="å¯ç”¨å¤šè½®ç¼–è¾‘æ¨¡å¼")
    parser.add_argument("--prompts_file", help="å¤šè½®ç¼–è¾‘æç¤ºè¯æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    parser.add_argument("--output_dir", default="multi_round_results", help="å¤šè½®ç¼–è¾‘è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.source_image):
        print(f"âŒ æºå›¾åƒä¸å­˜åœ¨: {args.source_image}")
        return
    
    print("ğŸ­ Kontext-inpaint ä¼ª mask-free æ¨ç†")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"ğŸ–¼ï¸ æºå›¾åƒ: {args.source_image}")
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    try:
        inferencer = KontextInpaintInference(args.model_path, base_model_path=args.base_model_path)
    except Exception as e:
        print(f"âŒ æ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    if args.multi_round and args.prompts_file:
        # å¤šè½®ç¼–è¾‘æ¨¡å¼
        print(f"ğŸ”„ å¤šè½®ç¼–è¾‘æ¨¡å¼")
        print(f"ğŸ“ æç¤ºè¯æ–‡ä»¶: {args.prompts_file}")
        
        if not os.path.exists(args.prompts_file):
            print(f"âŒ æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {args.prompts_file}")
            return
            
        # è¯»å–æç¤ºè¯
        with open(args.prompts_file, 'r', encoding='utf-8') as f:
            edit_prompts = [line.strip() for line in f if line.strip()]
        
        # æ‰§è¡Œå¤šè½®ç¼–è¾‘
        inferencer.multi_round_edit(
            source_path=args.source_image,
            edit_prompts=edit_prompts,
            output_dir=args.output_dir,
            num_inference_steps=args.steps,
            guidance_scale=args.guidance,
            seed=args.seed
        )
        
    else:
        # å•æ¬¡æ¨ç†æ¨¡å¼
        print(f"ğŸ’¬ ç¼–è¾‘æç¤º: {args.prompt}")
        
        # å‡†å¤‡å›¾åƒ
        try:
            source_image, white_control_image = inferencer.prepare_images(
                args.source_image, 
                target_size=(args.size, args.size)
            )
            print(f"âœ… å›¾åƒé¢„å¤„ç†å®Œæˆ: {args.size}x{args.size}")
        except Exception as e:
            print(f"âŒ å›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            return
        
        # è¿è¡Œæ¨ç†
        try:
            result_image = inferencer.run_inference(
                source_image=source_image,
                white_control_image=white_control_image,
                prompt=args.prompt,
                num_inference_steps=args.steps,
                guidance_scale=args.guidance,
                seed=args.seed
            )
            
            # ä¿å­˜ç»“æœ
            result_image.save(args.output)
            print(f"âœ… æ¨ç†å®Œæˆï¼ç»“æœä¿å­˜åˆ°: {args.output}")
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()