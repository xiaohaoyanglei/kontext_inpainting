#!/usr/bin/env python3
"""
æ•´ç†æ–°æ•°æ®é›†è„šæœ¬
å°†raw_data_testä¸­çš„emptyå’Œstagedæ–‡ä»¶åˆ†åˆ«æ”¾å…¥new_datasetçš„sourceå’Œtargetæ–‡ä»¶å¤¹
å¹¶æ·»åŠ ç»Ÿä¸€çš„promptæ–‡ä»¶
"""

import os
import shutil
from pathlib import Path

def prepare_new_dataset():
    """æ•´ç†æ–°æ•°æ®é›†"""
    print("ğŸš€ å¼€å§‹æ•´ç†æ–°æ•°æ®é›†...")
    
    # æºæ•°æ®ç›®å½•
    raw_data_dir = "/cloud/cloud-ssd1/raw_data_test"
    
    # æ–°æ•°æ®é›†ç›®å½•
    new_dataset_dir = "/cloud/cloud-ssd1/new_dataset"
    source_dir = os.path.join(new_dataset_dir, "source_images")
    target_dir = os.path.join(new_dataset_dir, "target_images")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(source_dir, exist_ok=True)
    os.makedirs(target_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰æ–‡ä»¶
    files = os.listdir(raw_data_dir)
    
    # åˆ†ç¦»emptyå’Œstagedæ–‡ä»¶
    empty_files = [f for f in files if f.endswith('_empty.png')]
    staged_files = [f for f in files if f.endswith('_staged.png')]
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(empty_files)} ä¸ªemptyæ–‡ä»¶")
    print(f"ğŸ“Š æ‰¾åˆ° {len(staged_files)} ä¸ªstagedæ–‡ä»¶")
    
    # å¤„ç†emptyæ–‡ä»¶ï¼ˆæºå›¾åƒï¼‰
    for empty_file in empty_files:
        # æå–åŸºç¡€åç§°ï¼ˆå»æ‰_empty.pngï¼‰
        base_name = empty_file.replace('_empty.png', '')
        
        # å¤åˆ¶åˆ°sourceç›®å½•
        source_path = os.path.join(source_dir, f"{base_name}.png")
        shutil.copy2(os.path.join(raw_data_dir, empty_file), source_path)
        
        # åˆ›å»ºå¯¹åº”çš„promptæ–‡ä»¶
        prompt_path = os.path.join(source_dir, f"{base_name}.txt")
        with open(prompt_path, 'w') as f:
            f.write("add furniture")
        
        print(f"  âœ… å¤„ç†æºå›¾åƒ: {empty_file} -> {base_name}.png")
    
    # å¤„ç†stagedæ–‡ä»¶ï¼ˆç›®æ ‡å›¾åƒï¼‰
    for staged_file in staged_files:
        # æå–åŸºç¡€åç§°ï¼ˆå»æ‰_staged.pngï¼‰
        base_name = staged_file.replace('_staged.png', '')
        
        # å¤åˆ¶åˆ°targetç›®å½•
        target_path = os.path.join(target_dir, f"{base_name}.png")
        shutil.copy2(os.path.join(raw_data_dir, staged_file), target_path)
        
        print(f"  âœ… å¤„ç†ç›®æ ‡å›¾åƒ: {staged_file} -> {base_name}.png")
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    print("\nğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    source_files = [f for f in os.listdir(source_dir) if f.endswith('.png')]
    target_files = [f for f in os.listdir(target_dir) if f.endswith('.png')]
    prompt_files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]
    
    print(f"  ğŸ“ source_images: {len(source_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    print(f"  ğŸ“ target_images: {len(target_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    print(f"  ğŸ“ promptæ–‡ä»¶: {len(prompt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
    
    # æ£€æŸ¥é…å¯¹å®Œæ•´æ€§
    source_names = {os.path.splitext(f)[0] for f in source_files}
    target_names = {os.path.splitext(f)[0] for f in target_files}
    prompt_names = {os.path.splitext(f)[0] for f in prompt_files}
    
    # æ‰¾åˆ°å®Œæ•´çš„é…å¯¹
    complete_pairs = source_names & target_names & prompt_names
    
    print(f"  âœ… å®Œæ•´é…å¯¹: {len(complete_pairs)} å¯¹")
    
    if len(complete_pairs) > 0:
        print(f"  ğŸ“‹ é…å¯¹ç¤ºä¾‹: {list(complete_pairs)[:5]}")
    
    # æ£€æŸ¥ç¼ºå¤±çš„æ–‡ä»¶
    missing_targets = source_names - target_names
    missing_prompts = source_names - prompt_names
    
    if missing_targets:
        print(f"  âš ï¸  ç¼ºå¤±ç›®æ ‡å›¾åƒ: {len(missing_targets)} ä¸ª")
        print(f"     ç¤ºä¾‹: {list(missing_targets)[:3]}")
    
    if missing_prompts:
        print(f"  âš ï¸  ç¼ºå¤±promptæ–‡ä»¶: {len(missing_prompts)} ä¸ª")
        print(f"     ç¤ºä¾‹: {list(missing_prompts)[:3]}")
    
    print(f"\nğŸ‰ æ–°æ•°æ®é›†æ•´ç†å®Œæˆ!")
    print(f"   æºæ•°æ®ç›®å½•: {raw_data_dir}")
    print(f"   æ–°æ•°æ®é›†ç›®å½•: {new_dataset_dir}")
    print(f"   å¯ç”¨è®­ç»ƒå¯¹: {len(complete_pairs)} å¯¹")
    
    return len(complete_pairs)

def verify_image_differences():
    """éªŒè¯æºå›¾å’Œç›®æ ‡å›¾æ˜¯å¦æœ‰æ˜æ˜¾å·®å¼‚"""
    print("\nğŸ” éªŒè¯å›¾åƒå·®å¼‚...")
    
    from PIL import Image
    import torch
    from torchvision import transforms
    
    source_dir = "/cloud/cloud-ssd1/new_dataset/source_images"
    target_dir = "/cloud/cloud-ssd1/new_dataset/target_images"
    
    # æ£€æŸ¥å‰5å¯¹å›¾åƒ
    source_files = sorted([f for f in os.listdir(source_dir) if f.endswith('.png')])[:5]
    
    transform = transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
    ])
    
    total_mse = 0
    count = 0
    
    for source_file in source_files:
        base_name = os.path.splitext(source_file)[0]
        target_file = f"{base_name}.png"
        target_path = os.path.join(target_dir, target_file)
        
        if os.path.exists(target_path):
            # åŠ è½½å›¾åƒ
            source_img = Image.open(os.path.join(source_dir, source_file)).convert('RGB')
            target_img = Image.open(target_path).convert('RGB')
            
            # è½¬æ¢ä¸ºtensor
            source_tensor = transform(source_img)
            target_tensor = transform(target_img)
            
            # è®¡ç®—å·®å¼‚
            mse = torch.mean((source_tensor - target_tensor) ** 2).item()
            similarity = 1 - mse
            
            total_mse += mse
            count += 1
            
            print(f"  {base_name}:")
            print(f"    MSE: {mse:.6f}")
            print(f"    ç›¸ä¼¼åº¦: {similarity:.6f}")
            print(f"    å·®å¼‚æ˜æ˜¾: {'æ˜¯' if similarity < 0.9 else 'å¦'}")
            print()
    
    if count > 0:
        avg_mse = total_mse / count
        avg_similarity = 1 - avg_mse
        print(f"  å¹³å‡MSE: {avg_mse:.6f}")
        print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.6f}")
        print(f"  æ•´ä½“å·®å¼‚: {'æ˜æ˜¾' if avg_similarity < 0.9 else 'ä¸æ˜æ˜¾'}")

if __name__ == "__main__":
    # æ•´ç†æ–°æ•°æ®é›†
    pair_count = prepare_new_dataset()
    
    # éªŒè¯å›¾åƒå·®å¼‚
    verify_image_differences()
    
    print(f"\nâœ… æ–°æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œå…±æœ‰ {pair_count} å¯¹è®­ç»ƒæ•°æ®")
    print("ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–°æ•°æ®é›†é‡æ–°è®­ç»ƒæ¨¡å‹äº†ï¼")
