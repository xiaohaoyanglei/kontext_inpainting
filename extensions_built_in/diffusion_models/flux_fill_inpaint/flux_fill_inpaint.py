import os
import torch
import torch.nn as nn
from typing import TYPE_CHECKING

from toolkit.config_modules import ModelConfig
from toolkit.print import print_acc
from toolkit.models.base_model import BaseModel
from diffusers import FluxTransformer2DModel, AutoencoderKL, FluxFillPipeline
from toolkit.basic import flush
from toolkit.prompt_utils import PromptEmbeds
from toolkit.samplers.custom_flowmatch_sampler import CustomFlowMatchEulerDiscreteScheduler
from toolkit.models.flux import add_model_gpu_splitter_to_flux, bypass_flux_guidance, restore_flux_guidance
from toolkit.dequantize import patch_dequantization_on_save
from toolkit.accelerator import get_accelerator, unwrap_model
from toolkit.util.quantize import quantize, get_qtype
from transformers import T5TokenizerFast, T5EncoderModel, CLIPTextModel, CLIPTokenizer
import torch.nn.functional as F

if TYPE_CHECKING:
    from toolkit.data_transfer_object.data_loader import DataLoaderBatchDTO

# ä½¿ç”¨ä¸ Kontext ç›¸åŒçš„è°ƒåº¦å™¨é…ç½®ä»¥è·å¾—å¤šè½®ä¸€è‡´æ€§
scheduler_config = {
    "base_image_seq_len": 256,
    "base_shift": 0.5,
    "max_image_seq_len": 4096,
    "max_shift": 1.15,
    "num_train_timesteps": 1000,
    "shift": 3.0,
    "use_dynamic_shifting": True
}


class FluxFillInpaintModel(BaseModel):
    """
    åŸºäº FLUX.1-Fill çš„ Kontext-inpaint æ¨¡å‹
    
    æ ¸å¿ƒè®¾è®¡ï¼š
    1. åŸºç¡€æ¨¡å‹ï¼šFLUX.1-Fill-dev å®Œæ•´ checkpointï¼ˆVAE + UNetï¼‰
    2. æ‰©å±•ï¼š32â†’hidden æŠ•å½±å±‚ï¼ˆå‰16é€šé“å¤åˆ¶Fillæƒé‡ï¼Œå16é€šé“ç½®é›¶ï¼‰
    3. æ¶æ„ï¼šå€Ÿç”¨ Kontext çš„ Flow-Matching + RoPE å®ç°å¤šè½®ä¸€è‡´æ€§
    4. æ•°æ®ï¼šWhiteMaskDatasetï¼ˆåŸå›¾ + çº¯ç™½æ§åˆ¶å›¾ï¼‰
    """
    arch = "flux_fill_inpaint"

    def __init__(
            self,
            device,
            model_config: ModelConfig,
            dtype='bf16',
            custom_pipeline=None,
            noise_scheduler=None,
            **kwargs
    ):
        # Kontext-inpaint ç‰¹å®šå‚æ•°
        self.kontext_inpaint_mode = model_config.model_kwargs.get('kontext_inpaint_mode', False)
        self.init_projection_from_original = model_config.model_kwargs.get('init_projection_from_original', True)
        self.two_stage_training = model_config.model_kwargs.get('two_stage_training', False)
        self.stage1_steps = model_config.model_kwargs.get('stage1_steps', 1000)
        
        super().__init__(
            device,
            model_config,
            dtype,
            custom_pipeline,
            noise_scheduler,
            **kwargs
        )
        
        self.is_flow_matching = True
        self.is_transformer = True
        self.target_lora_modules = ['FluxTransformer2DModel']
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.current_step = 0
        self.is_stage1 = True  # å½“å‰æ˜¯å¦åœ¨ç¬¬ä¸€é˜¶æ®µï¼ˆåªè®­ç»ƒprojectionï¼‰
        
        print_acc(f"ğŸ­ FluxFillInpaintModel åˆå§‹åŒ–å®Œæˆ")
        print_acc(f"   - åŸºäº: FLUX.1-Fill-dev")
        print_acc(f"   - Kontext-inpaint æ¨¡å¼: {self.kontext_inpaint_mode}")
        print_acc(f"   - æŠ•å½±å±‚åˆå§‹åŒ–: {self.init_projection_from_original}")
        print_acc(f"   - ä¸¤é˜¶æ®µè®­ç»ƒ: {self.two_stage_training}")
        if self.two_stage_training:
            print_acc(f"   - ç¬¬ä¸€é˜¶æ®µæ­¥æ•°: {self.stage1_steps}")

    @staticmethod
    def get_train_scheduler():
        """ä½¿ç”¨ä¸ Kontext ç›¸åŒçš„è°ƒåº¦å™¨é…ç½®"""
        return CustomFlowMatchEulerDiscreteScheduler(**scheduler_config)

    def get_bucket_divisibility(self):
        return 16

    def load_model(self):
        """åŠ è½½ FLUX.1-Fill æ¨¡å‹å¹¶æ·»åŠ  32â†’hidden æŠ•å½±å±‚"""
        dtype = self.torch_dtype
        self.print_and_status_update("Loading FLUX.1-Fill model for Kontext-inpaint")
        
        model_path = self.model_config.name_or_path
        base_model_path = self.model_config.extras_name_or_path or model_path

        # åŠ è½½ Transformerï¼ˆåŸºäº FLUX.1-Fillï¼‰
        transformer_path = model_path
        transformer_subfolder = 'transformer'
        if os.path.exists(transformer_path):
            transformer_subfolder = None
            transformer_path = os.path.join(transformer_path, 'transformer')
            # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´checkpoint
            te_folder_path = os.path.join(model_path, 'text_encoder')
            if os.path.exists(te_folder_path):
                base_model_path = model_path

        self.print_and_status_update("Loading FLUX.1-Fill transformer")
        transformer = FluxTransformer2DModel.from_pretrained(
            transformer_path,
            subfolder=transformer_subfolder,
            torch_dtype=dtype
        )
        transformer.to(self.quantize_device, dtype=dtype)

        if self.model_config.quantize:
            patch_dequantization_on_save(transformer)
            quantization_type = get_qtype(self.model_config.qtype)
            self.print_and_status_update("Quantizing transformer")
            quantize(transformer, weights=quantization_type,
                     **self.model_config.quantize_kwargs)
            transformer.to(self.device_torch)
        else:
            transformer.to(self.device_torch, dtype=dtype)

        flush()

        # åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼ˆä½¿ç”¨ FLUX.1-Fill çš„é…ç½®ï¼‰
        self.print_and_status_update("Loading T5")
        tokenizer_2 = T5TokenizerFast.from_pretrained(
            base_model_path, subfolder="tokenizer_2", torch_dtype=dtype
        )
        text_encoder_2 = T5EncoderModel.from_pretrained(
            base_model_path, subfolder="text_encoder_2", torch_dtype=dtype
        )
        text_encoder_2.to(self.device_torch, dtype=dtype)
        flush()

        if self.model_config.quantize_te:
            self.print_and_status_update("Quantizing T5")
            quantize(text_encoder_2, weights=get_qtype(
                self.model_config.qtype))
            text_encoder_2.to(self.device_torch)
            flush()

        self.print_and_status_update("Loading CLIP")
        text_encoder = CLIPTextModel.from_pretrained(
            base_model_path, subfolder="text_encoder", torch_dtype=dtype)
        tokenizer = CLIPTokenizer.from_pretrained(
            base_model_path, subfolder="tokenizer", torch_dtype=dtype)
        text_encoder.to(self.device_torch, dtype=dtype)

        self.print_and_status_update("Loading VAE")
        vae = AutoencoderKL.from_pretrained(
            base_model_path, subfolder="vae", torch_dtype=dtype)

        # ä½¿ç”¨ Kontext é£æ ¼çš„è°ƒåº¦å™¨
        self.noise_scheduler = FluxFillInpaintModel.get_train_scheduler()

        self.print_and_status_update("Making pipeline")
        # ä½¿ç”¨ FluxFillPipeline è€Œä¸æ˜¯ FluxKontextPipeline
        pipe: FluxFillPipeline = FluxFillPipeline(
            scheduler=self.noise_scheduler,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            text_encoder_2=text_encoder_2,
            tokenizer_2=tokenizer_2,
            vae=vae,
            transformer=transformer,
        )

        self.print_and_status_update("Preparing Model")

        text_encoder = [pipe.text_encoder, pipe.text_encoder_2]
        tokenizer = [pipe.tokenizer, pipe.tokenizer_2]

        pipe.transformer = pipe.transformer.to(self.device_torch)

        flush()
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        text_encoder[0].to(self.device_torch)
        text_encoder[0].requires_grad_(False)
        text_encoder[0].eval()
        text_encoder[1].to(self.device_torch)
        text_encoder[1].requires_grad_(False)
        text_encoder[1].eval()
        pipe.transformer = pipe.transformer.to(self.device_torch)
        flush()

        # ä¿å­˜åˆ°æ¨¡å‹ç±»
        self.vae = vae
        self.text_encoder = text_encoder
        self.tokenizer = tokenizer
        self.model = pipe.transformer
        self.pipeline = pipe
        
        # åº”ç”¨ Kontext-inpaint ä¿®æ”¹
        if self.kontext_inpaint_mode and self.init_projection_from_original:
            self._init_kontext_inpaint_projection()
            
        if self.two_stage_training:
            self._setup_two_stage_training()
            
        self.print_and_status_update("FLUX.1-Fill Kontext-inpaint Model Loaded")

    def _init_kontext_inpaint_projection(self):
        """
        åˆå§‹åŒ– 32â†’hidden æŠ•å½±å±‚
        ç­–ç•¥ï¼šå‰16é€šé“å¤åˆ¶ FLUX.1-Fill çš„åŸå§‹æƒé‡ï¼Œå16é€šé“ç½®é›¶
        """
        transformer: FluxTransformer2DModel = self.model
        
        # è·å–åŸå§‹çš„ x_embedder (16â†’hidden)
        original_embedder = transformer.x_embedder
        original_in_channels = original_embedder.weight.shape[1]  # åº”è¯¥æ˜¯16*4=64 (16é€šé“*4patch)
        hidden_size = original_embedder.weight.shape[0]
        
        print_acc(f"ğŸ”§ åˆå§‹åŒ–åŸºäº FLUX.1-Fill çš„ Kontext-inpaint æŠ•å½±å±‚:")
        print_acc(f"   - FLUX.1-Fill åŸå§‹è¾“å…¥é€šé“: {original_in_channels}")
        print_acc(f"   - Kontext-inpaint ç›®æ ‡è¾“å…¥é€šé“: {original_in_channels * 2} (32é€šé“)")
        print_acc(f"   - éšè—å±‚ç»´åº¦: {hidden_size}")
        
        # åˆ›å»ºæ–°çš„ 32â†’hidden æŠ•å½±å±‚
        new_in_channels = original_in_channels * 2  # 32é€šé“ * 4patch = 128
        new_embedder = nn.Linear(new_in_channels, hidden_size, bias=True)
        
        # åˆå§‹åŒ–æƒé‡ï¼šå‰16é€šé“å¤åˆ¶ FLUX.1-Fill æƒé‡ï¼Œå16é€šé“ç½®é›¶
        with torch.no_grad():
            # å‰åŠéƒ¨åˆ†ï¼šå¤åˆ¶ FLUX.1-Fill åŸå§‹æƒé‡
            new_embedder.weight[:, :original_in_channels].copy_(original_embedder.weight)
            # ååŠéƒ¨åˆ†ï¼šç½®é›¶ï¼ˆå¯¹åº”çº¯ç™½æ§åˆ¶å›¾çš„é€šé“ï¼‰
            new_embedder.weight[:, original_in_channels:].zero_()
            
            # åç½®å¤åˆ¶ - æ­£ç¡®å¤„ç† nn.Parameter
            if original_embedder.bias is not None:
                new_embedder.bias.copy_(original_embedder.bias)
            else:
                new_embedder.bias.zero_()
        
        # æ›¿æ¢æ¨¡å‹çš„æŠ•å½±å±‚
        transformer.x_embedder = new_embedder.to(self.device_torch, dtype=self.torch_dtype)
        
        print_acc(f"âœ… åŸºäº FLUX.1-Fill çš„æŠ•å½±å±‚åˆå§‹åŒ–å®Œæˆ")
        print_acc(f"   - æ–°æŠ•å½±å±‚æƒé‡å½¢çŠ¶: {new_embedder.weight.shape}")
        print_acc(f"   - å‰16é€šé“: å¤åˆ¶ FLUX.1-Fill æƒé‡")
        print_acc(f"   - å16é€šé“: ç½®é›¶åˆå§‹åŒ–")

    def _setup_two_stage_training(self):
        """è®¾ç½®ä¸¤é˜¶æ®µè®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µåªè®­ç»ƒprojectionå±‚"""
        self._freeze_all_except_projection()
        print_acc(f"ğŸ”„ ä¸¤é˜¶æ®µè®­ç»ƒè®¾ç½®:")
        print_acc(f"   - ç¬¬ä¸€é˜¶æ®µ: åªè®­ç»ƒ x_embedder æŠ•å½±å±‚")
        print_acc(f"   - ç¬¬äºŒé˜¶æ®µ: å…¨æ¨¡å‹å¾®è°ƒ")
    
    def _freeze_all_except_projection(self):
        """å†»ç»“é™¤æŠ•å½±å±‚å¤–çš„æ‰€æœ‰å‚æ•°"""
        transformer: FluxTransformer2DModel = self.model
        
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in transformer.parameters():
            param.requires_grad = False
        
        # åªè§£å†» x_embedder
        for param in transformer.x_embedder.parameters():
            param.requires_grad = True
            
        print_acc(f"â„ï¸  ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“å…¨æ¨¡å‹ï¼Œåªè®­ç»ƒ x_embedder")
    
    def _unfreeze_all_parameters(self):
        """è§£å†»æ‰€æœ‰å‚æ•°ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰"""
        transformer: FluxTransformer2DModel = self.model
        
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in transformer.parameters():
            param.requires_grad = True
            
        print_acc(f"ğŸ”¥ ç¬¬äºŒé˜¶æ®µï¼šè§£å†»å…¨æ¨¡å‹å‚æ•°")
    
    def update_training_stage(self, current_step: int):
        """
        æ›´æ–°è®­ç»ƒé˜¶æ®µ
        Args:
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°
        """
        self.current_step = current_step
        
        if self.two_stage_training:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µ
            if self.is_stage1 and current_step >= self.stage1_steps:
                print_acc(f"ğŸ”„ åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µè®­ç»ƒ (æ­¥æ•°: {current_step})")
                self._unfreeze_all_parameters()
                self.is_stage1 = False
                return True  # è¿”å›Trueè¡¨ç¤ºé˜¶æ®µåˆ‡æ¢
        
        return False

    def condition_noisy_latents(self, latents: torch.Tensor, batch: 'DataLoaderBatchDTO'):
        """
        æ¡ä»¶å¤„ç†ï¼š32é€šé“è¾“å…¥ï¼ˆåŸå›¾16 + çº¯ç™½æ§åˆ¶å›¾16ï¼‰
        åŸºäº FLUX.1-Fill çš„å¤„ç†é€»è¾‘ï¼Œæ‰©å±•ä¸º32é€šé“
        """
        with torch.no_grad():
            control_tensor = batch.control_tensor
            if control_tensor is not None:
                self.vae.to(self.device_torch)
                # é¢„å¤„ç†æ§åˆ¶å¼ é‡ï¼ˆçº¯ç™½å›¾åƒï¼‰
                control_tensor = control_tensor * 2 - 1  # [0,1] -> [-1,1]
                control_tensor = control_tensor.to(self.vae_device_torch, dtype=self.torch_dtype)
                
                # è°ƒæ•´å°ºå¯¸ä»¥åŒ¹é… latents
                if batch.tensor is not None:
                    target_h, target_w = batch.tensor.shape[2], batch.tensor.shape[3]
                else:
                    target_h = batch.file_items[0].crop_height
                    target_w = batch.file_items[0].crop_width

                if control_tensor.shape[2] != target_h or control_tensor.shape[3] != target_w:
                    control_tensor = F.interpolate(
                        control_tensor, size=(target_h, target_w), mode='bilinear'
                    )
                    
                # VAE ç¼–ç æ§åˆ¶å›¾åƒ
                control_latent = self.encode_images(control_tensor).to(latents.device, latents.dtype)
                
                # æ‹¼æ¥ï¼šåŸå›¾latent(16) + æ§åˆ¶å›¾latent(16) = 32é€šé“
                latents = torch.cat((latents, control_latent), dim=1)
                
                print_acc(f"ğŸ­ FLUX.1-Fill Kontext-inpaint latent æ‹¼æ¥:")
                print_acc(f"   - åŸå›¾ latent: 16 é€šé“")
                print_acc(f"   - çº¯ç™½æ§åˆ¶å›¾ latent: 16 é€šé“") 
                print_acc(f"   - æœ€ç»ˆè¾“å…¥: {latents.shape[1]} é€šé“")

        return latents.detach()

    def get_base_model_version(self):
        return "flux.1_fill_inpaint"

    # ä»¥ä¸‹æ–¹æ³•ç»§æ‰¿ BaseModel çš„é»˜è®¤å®ç°ï¼Œä½†ä½¿ç”¨ Fill çš„é€»è¾‘
    def get_generation_pipeline(self):
        scheduler = FluxFillInpaintModel.get_train_scheduler()

        pipeline: FluxFillPipeline = FluxFillPipeline(
            scheduler=scheduler,
            text_encoder=unwrap_model(self.text_encoder[0]),
            tokenizer=self.tokenizer[0],
            text_encoder_2=unwrap_model(self.text_encoder[1]),
            tokenizer_2=self.tokenizer[1],
            vae=unwrap_model(self.vae),
            transformer=unwrap_model(self.transformer)
        )

        pipeline = pipeline.to(self.device_torch)
        return pipeline

    def get_model_has_grad(self):
        return self.model.proj_out.weight.requires_grad

    def get_te_has_grad(self):
        return self.text_encoder[1].encoder.block[0].layer[0].SelfAttention.q.weight.requires_grad
    
    def get_prompt_embeds(self, prompt: str) -> PromptEmbeds:
        """å®ç°æç¤ºè¯ç¼–ç ï¼ŒåŸºäº FLUX.1-Fill çš„åŒæ–‡æœ¬ç¼–ç å™¨"""
        if self.pipeline.text_encoder.device != self.device_torch:
            self.pipeline.text_encoder.to(self.device_torch)
        
        from toolkit import train_tools
        prompt_embeds, pooled_prompt_embeds = train_tools.encode_prompts_flux(
            self.tokenizer,
            self.text_encoder,
            prompt,
            max_length=512,
        )
        pe = PromptEmbeds(
            prompt_embeds
        )
        pe.pooled_embeds = pooled_prompt_embeds
        return pe