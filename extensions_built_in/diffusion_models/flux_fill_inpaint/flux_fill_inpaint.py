import os
import torch
import torch.nn as nn
from typing import TYPE_CHECKING

from toolkit.config_modules import ModelConfig
from toolkit.print import print_acc
from toolkit.models.base_model import BaseModel
from diffusers import FluxTransformer2DModel, AutoencoderKL, FluxFillPipeline
from toolkit.basic import flush
from toolkit.prompt_utils import PromptEmbeds
from toolkit.samplers.custom_flowmatch_sampler import CustomFlowMatchEulerDiscreteScheduler
from toolkit.models.flux import add_model_gpu_splitter_to_flux, bypass_flux_guidance, restore_flux_guidance
from toolkit.dequantize import patch_dequantization_on_save
from toolkit.accelerator import get_accelerator, unwrap_model
from toolkit.util.quantize import quantize, get_qtype
from transformers import T5TokenizerFast, T5EncoderModel, CLIPTextModel, CLIPTokenizer
import torch.nn.functional as F

if TYPE_CHECKING:
    from toolkit.data_transfer_object.data_loader import DataLoaderBatchDTO

# ä½¿ç”¨ä¸ Kontext ç›¸åŒçš„è°ƒåº¦å™¨é…ç½®ä»¥è·å¾—å¤šè½®ä¸€è‡´æ€§
scheduler_config = {
    "base_image_seq_len": 256,
    "base_shift": 0.5,
    "max_image_seq_len": 4096,
    "max_shift": 1.15,
    "num_train_timesteps": 1000,
    "shift": 3.0,
    "use_dynamic_shifting": True
}


class FluxFillInpaintModel(BaseModel):
    """
    åŸºäº FLUX.1-Fill çš„ Kontext-inpaint æ¨¡å‹
    
    æ ¸å¿ƒè®¾è®¡ï¼š
    1. åŸºç¡€æ¨¡å‹ï¼šFLUX.1-Fill-dev å®Œæ•´ checkpointï¼ˆVAE + UNetï¼‰
    2. æ‰©å±•ï¼š32â†’hidden æŠ•å½±å±‚ï¼ˆå‰16é€šé“å¤åˆ¶Fillæƒé‡ï¼Œå16é€šé“ç½®é›¶ï¼‰
    3. æ¶æ„ï¼šå€Ÿç”¨ Kontext çš„ Flow-Matching + RoPE å®ç°å¤šè½®ä¸€è‡´æ€§
    4. æ•°æ®ï¼šWhiteMaskDatasetï¼ˆåŸå›¾ + çº¯ç™½æ§åˆ¶å›¾ï¼‰
    """
    arch = "flux_fill_inpaint"

    def __init__(
            self,
            device,
            model_config: ModelConfig,
            dtype='bf16',
            custom_pipeline=None,
            noise_scheduler=None,
            **kwargs
    ):
        # Kontext-inpaint ç‰¹å®šå‚æ•°
        self.kontext_inpaint_mode = model_config.model_kwargs.get('kontext_inpaint_mode', False)
        self.init_projection_from_original = model_config.model_kwargs.get('init_projection_from_original', True)
        self.two_stage_training = model_config.model_kwargs.get('two_stage_training', False)
        self.stage1_steps = model_config.model_kwargs.get('stage1_steps', 1000)
        
        super().__init__(
            device,
            model_config,
            dtype,
            custom_pipeline,
            noise_scheduler,
            **kwargs
        )
        
        self.is_flow_matching = True
        self.is_transformer = True
        self.target_lora_modules = ['FluxTransformer2DModel']
        
        # è®­ç»ƒçŠ¶æ€è·Ÿè¸ª
        self.current_step = 0
        self.is_stage1 = True  # å½“å‰æ˜¯å¦åœ¨ç¬¬ä¸€é˜¶æ®µï¼ˆåªè®­ç»ƒprojectionï¼‰
        
        print_acc(f"ğŸ­ FluxFillInpaintModel åˆå§‹åŒ–å®Œæˆ")
        print_acc(f"   - åŸºäº: FLUX.1-Fill-dev")
        print_acc(f"   - Kontext-inpaint æ¨¡å¼: {self.kontext_inpaint_mode}")
        print_acc(f"   - æŠ•å½±å±‚åˆå§‹åŒ–: {self.init_projection_from_original}")
        print_acc(f"   - ä¸¤é˜¶æ®µè®­ç»ƒ: {self.two_stage_training}")
        if self.two_stage_training:
            print_acc(f"   - ç¬¬ä¸€é˜¶æ®µæ­¥æ•°: {self.stage1_steps}")

    @staticmethod
    def get_train_scheduler():
        """ä½¿ç”¨ä¸ Kontext ç›¸åŒçš„è°ƒåº¦å™¨é…ç½®"""
        return CustomFlowMatchEulerDiscreteScheduler(**scheduler_config)

    def get_bucket_divisibility(self):
        return 16

    def load_model(self):
        """åŠ è½½ FLUX.1-Fill æ¨¡å‹å¹¶æ·»åŠ  32â†’hidden æŠ•å½±å±‚"""
        dtype = self.torch_dtype
        self.print_and_status_update("Loading FLUX.1-Fill model for Kontext-inpaint")
        
        model_path = self.model_config.name_or_path
        base_model_path = self.model_config.extras_name_or_path or model_path

        # åŠ è½½ Transformerï¼ˆåŸºäº FLUX.1-Fillï¼‰
        transformer_path = model_path
        transformer_subfolder = 'transformer'
        if os.path.exists(transformer_path):
            transformer_subfolder = None
            transformer_path = os.path.join(transformer_path, 'transformer')
            # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´checkpoint
            te_folder_path = os.path.join(model_path, 'text_encoder')
            if os.path.exists(te_folder_path):
                base_model_path = model_path

        self.print_and_status_update("Loading FLUX.1-Fill transformer")
        transformer = FluxTransformer2DModel.from_pretrained(
            transformer_path,
            subfolder=transformer_subfolder,
            torch_dtype=dtype
        )
        transformer.to(self.quantize_device, dtype=dtype)

        if self.model_config.quantize:
            patch_dequantization_on_save(transformer)
            quantization_type = get_qtype(self.model_config.qtype)
            self.print_and_status_update("Quantizing transformer")
            quantize(transformer, weights=quantization_type,
                     **self.model_config.quantize_kwargs)
            transformer.to(self.device_torch)
        else:
            transformer.to(self.device_torch, dtype=dtype)

        flush()

        # åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼ˆä½¿ç”¨ FLUX.1-Fill çš„é…ç½®ï¼‰
        self.print_and_status_update("Loading T5")
        tokenizer_2 = T5TokenizerFast.from_pretrained(
            base_model_path, subfolder="tokenizer_2", torch_dtype=dtype
        )
        text_encoder_2 = T5EncoderModel.from_pretrained(
            base_model_path, subfolder="text_encoder_2", torch_dtype=dtype
        )
        text_encoder_2.to(self.device_torch, dtype=dtype)
        flush()

        if self.model_config.quantize_te:
            self.print_and_status_update("Quantizing T5")
            quantize(text_encoder_2, weights=get_qtype(
                self.model_config.qtype))
            text_encoder_2.to(self.device_torch)
            flush()

        self.print_and_status_update("Loading CLIP")
        text_encoder = CLIPTextModel.from_pretrained(
            base_model_path, subfolder="text_encoder", torch_dtype=dtype)
        tokenizer = CLIPTokenizer.from_pretrained(
            base_model_path, subfolder="tokenizer", torch_dtype=dtype)
        text_encoder.to(self.device_torch, dtype=dtype)

        self.print_and_status_update("Loading VAE")
        vae = AutoencoderKL.from_pretrained(
            base_model_path, subfolder="vae", torch_dtype=dtype)

        # ä½¿ç”¨ Kontext é£æ ¼çš„è°ƒåº¦å™¨
        self.noise_scheduler = FluxFillInpaintModel.get_train_scheduler()

        self.print_and_status_update("Making pipeline")
        # ä½¿ç”¨ FluxFillPipeline è€Œä¸æ˜¯ FluxKontextPipeline
        pipe: FluxFillPipeline = FluxFillPipeline(
            scheduler=self.noise_scheduler,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            text_encoder_2=text_encoder_2,
            tokenizer_2=tokenizer_2,
            vae=vae,
            transformer=transformer,
        )

        self.print_and_status_update("Preparing Model")

        text_encoder = [pipe.text_encoder, pipe.text_encoder_2]
        tokenizer = [pipe.tokenizer, pipe.tokenizer_2]

        pipe.transformer = pipe.transformer.to(self.device_torch)

        flush()
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        text_encoder[0].to(self.device_torch)
        text_encoder[0].requires_grad_(False)
        text_encoder[0].eval()
        text_encoder[1].to(self.device_torch)
        text_encoder[1].requires_grad_(False)
        text_encoder[1].eval()
        pipe.transformer = pipe.transformer.to(self.device_torch)
        flush()

        # ä¿å­˜åˆ°æ¨¡å‹ç±»
        self.vae = vae
        self.text_encoder = text_encoder
        self.tokenizer = tokenizer
        self.model = pipe.transformer
        self.pipeline = pipe
        
        # åº”ç”¨ Kontext-inpaint ä¿®æ”¹
        if self.kontext_inpaint_mode and self.init_projection_from_original:
            self._init_kontext_inpaint_projection()
            
        if self.two_stage_training:
            self._setup_two_stage_training()
            
        self.print_and_status_update("FLUX.1-Fill Kontext-inpaint Model Loaded")

    def _init_kontext_inpaint_projection(self):
        """
        åˆå§‹åŒ– 32â†’hidden æŠ•å½±å±‚
        ç­–ç•¥ï¼šå‰16é€šé“å¤åˆ¶ FLUX.1-Fill çš„åŸå§‹æƒé‡ï¼Œå16é€šé“ç½®é›¶
        """
        transformer: FluxTransformer2DModel = self.model
        
        # è·å–åŸå§‹çš„ x_embedder (16â†’hidden)
        original_embedder = transformer.x_embedder
        original_in_channels = original_embedder.weight.shape[1]  # FLUX.1-Fill: 64 (16é€šé“*4patch)
        hidden_size = original_embedder.weight.shape[0]
        
        print_acc(f"ğŸ”§ åˆå§‹åŒ–åŸºäº FLUX.1-Fill çš„ Kontext-inpaint æŠ•å½±å±‚:")
        print_acc(f"   - FLUX.1-Fill åŸå§‹è¾“å…¥é€šé“: {original_in_channels}")
        print_acc(f"   - Kontext-inpaint ç›®æ ‡è¾“å…¥é€šé“: 128 (32é€šé“)")
        print_acc(f"   - éšè—å±‚ç»´åº¦: {hidden_size}")
        
        # åˆ›å»ºæ–°çš„ 32â†’hidden æŠ•å½±å±‚ 
        # å…³é”®ä¿®å¤: 32é€šé“ * 4patches = 128ï¼Œä¸æ˜¯ original_in_channels * 2
        new_in_channels = 128  # 32é€šé“ * 4patch = 128
        new_embedder = nn.Linear(new_in_channels, hidden_size, bias=True)
        
        # åˆå§‹åŒ–æƒé‡ï¼šå‰16é€šé“å¤åˆ¶ FLUX.1-Fill æƒé‡ï¼Œå16é€šé“ç½®é›¶
        with torch.no_grad():
            # è®¡ç®—éœ€è¦å¤åˆ¶çš„ç»´åº¦: æ–°æƒé‡å‰64ç»´ = åŸå§‹æƒé‡çš„å‰64ç»´
            half_channels = min(original_in_channels, 64)  # å‰16é€šé“å¯¹åº”çš„64ä¸ªç‰¹å¾
            
            # å‰åŠéƒ¨åˆ†ï¼šå¤åˆ¶ FLUX.1-Fill åŸå§‹æƒé‡
            new_embedder.weight[:, :half_channels].copy_(original_embedder.weight[:, :half_channels])
            # ååŠéƒ¨åˆ†ï¼šç½®é›¶ï¼ˆå¯¹åº”çº¯ç™½æ§åˆ¶å›¾çš„é€šé“ï¼‰
            new_embedder.weight[:, half_channels:].zero_()
            
            # åç½®å¤åˆ¶ - æ­£ç¡®å¤„ç† nn.Parameter
            if original_embedder.bias is not None:
                new_embedder.bias.copy_(original_embedder.bias)
            else:
                new_embedder.bias.zero_()
        
        # æ›¿æ¢æ¨¡å‹çš„æŠ•å½±å±‚
        transformer.x_embedder = new_embedder.to(self.device_torch, dtype=self.torch_dtype)
        
        print_acc(f"âœ… åŸºäº FLUX.1-Fill çš„æŠ•å½±å±‚åˆå§‹åŒ–å®Œæˆ")
        print_acc(f"   - æ–°æŠ•å½±å±‚æƒé‡å½¢çŠ¶: {new_embedder.weight.shape}")
        print_acc(f"   - å‰16é€šé“: å¤åˆ¶ FLUX.1-Fill æƒé‡")
        print_acc(f"   - å16é€šé“: ç½®é›¶åˆå§‹åŒ–")

    def _setup_two_stage_training(self):
        """è®¾ç½®ä¸¤é˜¶æ®µè®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µåªè®­ç»ƒprojectionå±‚"""
        self._freeze_all_except_projection()
        print_acc(f"ğŸ”„ ä¸¤é˜¶æ®µè®­ç»ƒè®¾ç½®:")
        print_acc(f"   - ç¬¬ä¸€é˜¶æ®µ: åªè®­ç»ƒ x_embedder æŠ•å½±å±‚")
        print_acc(f"   - ç¬¬äºŒé˜¶æ®µ: å…¨æ¨¡å‹å¾®è°ƒ")
    
    def _freeze_all_except_projection(self):
        """å†»ç»“é™¤æŠ•å½±å±‚å¤–çš„æ‰€æœ‰å‚æ•°"""
        transformer: FluxTransformer2DModel = self.model
        
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in transformer.parameters():
            param.requires_grad = False
        
        # åªè§£å†» x_embedder
        for param in transformer.x_embedder.parameters():
            param.requires_grad = True
            
        print_acc(f"â„ï¸  ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“å…¨æ¨¡å‹ï¼Œåªè®­ç»ƒ x_embedder")
    
    def _unfreeze_all_parameters(self):
        """è§£å†»æ‰€æœ‰å‚æ•°ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰"""
        transformer: FluxTransformer2DModel = self.model
        
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in transformer.parameters():
            param.requires_grad = True
            
        print_acc(f"ğŸ”¥ ç¬¬äºŒé˜¶æ®µï¼šè§£å†»å…¨æ¨¡å‹å‚æ•°")
    
    def update_training_stage(self, current_step: int):
        """
        æ›´æ–°è®­ç»ƒé˜¶æ®µ
        Args:
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°
        """
        self.current_step = current_step
        
        if self.two_stage_training:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µ
            if self.is_stage1 and current_step >= self.stage1_steps:
                print_acc(f"ğŸ”„ åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µè®­ç»ƒ (æ­¥æ•°: {current_step})")
                self._unfreeze_all_parameters()
                self.is_stage1 = False
                return True  # è¿”å›Trueè¡¨ç¤ºé˜¶æ®µåˆ‡æ¢
        
        return False

    def condition_noisy_latents(self, latents: torch.Tensor, batch: 'DataLoaderBatchDTO'):
        """
        æ¡ä»¶å¤„ç†ï¼š32é€šé“è¾“å…¥ï¼ˆåŸå›¾16 + çº¯ç™½æ§åˆ¶å›¾16ï¼‰
        åŸºäº FLUX.1-Fill çš„å¤„ç†é€»è¾‘ï¼Œæ‰©å±•ä¸º32é€šé“
        """
        with torch.no_grad():
            control_tensor = batch.control_tensor
            if control_tensor is not None:
                self.vae.to(self.device_torch)
                # é¢„å¤„ç†æ§åˆ¶å¼ é‡ï¼ˆçº¯ç™½å›¾åƒï¼‰
                control_tensor = control_tensor * 2 - 1  # [0,1] -> [-1,1]
                control_tensor = control_tensor.to(self.vae_device_torch, dtype=self.torch_dtype)
                
                # è°ƒæ•´å°ºå¯¸ä»¥åŒ¹é… latents
                if batch.tensor is not None:
                    target_h, target_w = batch.tensor.shape[2], batch.tensor.shape[3]
                else:
                    target_h = batch.file_items[0].crop_height
                    target_w = batch.file_items[0].crop_width

                if control_tensor.shape[2] != target_h or control_tensor.shape[3] != target_w:
                    control_tensor = F.interpolate(
                        control_tensor, size=(target_h, target_w), mode='bilinear'
                    )
                    
                # VAE ç¼–ç æ§åˆ¶å›¾åƒ
                control_latent = self.encode_images(control_tensor).to(latents.device, latents.dtype)
                
                # æ‹¼æ¥ï¼šåŸå›¾latent(16) + æ§åˆ¶å›¾latent(16) = 32é€šé“
                latents = torch.cat((latents, control_latent), dim=1)

        return latents.detach()

    def get_base_model_version(self):
        return "flux.1_fill_inpaint"

    # ä»¥ä¸‹æ–¹æ³•ç»§æ‰¿ BaseModel çš„é»˜è®¤å®ç°ï¼Œä½†ä½¿ç”¨ Fill çš„é€»è¾‘
    def get_generation_pipeline(self):
        scheduler = FluxFillInpaintModel.get_train_scheduler()

        pipeline: FluxFillPipeline = FluxFillPipeline(
            scheduler=scheduler,
            text_encoder=unwrap_model(self.text_encoder[0]),
            tokenizer=self.tokenizer[0],
            text_encoder_2=unwrap_model(self.text_encoder[1]),
            tokenizer_2=self.tokenizer[1],
            vae=unwrap_model(self.vae),
            transformer=unwrap_model(self.transformer)
        )

        pipeline = pipeline.to(self.device_torch)
        return pipeline

    def get_model_has_grad(self):
        return self.model.proj_out.weight.requires_grad

    def get_te_has_grad(self):
        return self.text_encoder[1].encoder.block[0].layer[0].SelfAttention.q.weight.requires_grad
    
    def get_prompt_embeds(self, prompt: str) -> PromptEmbeds:
        """å®ç°æç¤ºè¯ç¼–ç ï¼ŒåŸºäº FLUX.1-Fill çš„åŒæ–‡æœ¬ç¼–ç å™¨"""
        if self.pipeline.text_encoder.device != self.device_torch:
            self.pipeline.text_encoder.to(self.device_torch)
        
        from toolkit import train_tools
        prompt_embeds, pooled_prompt_embeds = train_tools.encode_prompts_flux(
            self.tokenizer,
            self.text_encoder,
            prompt,
            max_length=512,
        )
        pe = PromptEmbeds(
            prompt_embeds
        )
        pe.pooled_embeds = pooled_prompt_embeds
        return pe

    def generate_single_image(
        self,
        pipeline,
        gen_config: 'GenerateImageConfig',
        conditional_embeds: PromptEmbeds,
        unconditional_embeds: PromptEmbeds,
        generator: torch.Generator,
        extra: dict,
    ):
        """ç”Ÿæˆå•å¼ å›¾åƒ - ä½¿ç”¨FluxFill pipelineè¿›è¡Œproperé‡‡æ ·"""
        from PIL import Image
        import torch
        
        # ç¡®ä¿å°ºå¯¸æ˜¯16çš„å€æ•°
        gen_config.width = int(gen_config.width // 16 * 16)
        gen_config.height = int(gen_config.height // 16 * 16)
        
        try:
            # åˆ›å»ºç™½è‰²æºå›¾åƒå’Œç™½è‰²maskä½œä¸ºæµ‹è¯•è¾“å…¥
            white_source = Image.new('RGB', (gen_config.width, gen_config.height), (255, 255, 255))
            white_mask = Image.new('RGB', (gen_config.width, gen_config.height), (255, 255, 255))
            
            # ç¡®ä¿ generator åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if generator.device.type != self.device_torch.type:
                generator = torch.Generator(device=self.device_torch).manual_seed(generator.initial_seed())
            
            # ä½¿ç”¨FluxFill pipelineè¿›è¡Œæ¨ç†
            with torch.no_grad():
                result = pipeline(
                    prompt=gen_config.prompt,
                    image=white_source,
                    mask_image=white_mask,
                    num_inference_steps=getattr(gen_config, 'sample_steps', 20),
                    guidance_scale=gen_config.guidance_scale,
                    generator=generator,
                    height=gen_config.height,
                    width=gen_config.width,
                )
                
                return result.images[0]
                
        except Exception as e:
            print(f"âš ï¸ Pipelineæ¨ç†å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•ç”Ÿæˆ: {e}")
            # å¦‚æœpipelineå¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
            from PIL import Image, ImageDraw, ImageFont
            
            img = Image.new('RGB', (gen_config.width, gen_config.height), (240, 240, 240))
            draw = ImageDraw.Draw(img)
            
            # ç»˜åˆ¶ä¸€äº›æµ‹è¯•æ–‡æœ¬
            try:
                # å°è¯•ä½¿ç”¨ç³»ç»Ÿå­—ä½“
                font = ImageFont.load_default()
                text = f"Test Sample\n{gen_config.prompt[:30]}..."
                draw.text((10, 10), text, fill=(100, 100, 100), font=font)
            except:
                # å¦‚æœå­—ä½“åŠ è½½å¤±è´¥ï¼Œå°±ç”»ä¸€ä¸ªç®€å•çš„çŸ©å½¢
                draw.rectangle([50, 50, gen_config.width-50, gen_config.height-50], 
                             outline=(100, 100, 100), width=2)
            
            return img

    def get_noise_prediction(
        self,
        latent_model_input: torch.Tensor,
        timestep: torch.Tensor,
        text_embeddings: PromptEmbeds,
        guidance_embedding_scale: float,
        bypass_guidance_embedding: bool,
        **kwargs
    ):
        """è·å–å™ªå£°é¢„æµ‹ - å¤„ç†32é€šé“è¾“å…¥çš„æ ¸å¿ƒé€»è¾‘"""
        with torch.no_grad():
            bs, c, h, w = latent_model_input.shape
            
            # è‡ªåŠ¨å¤„ç†ï¼š16é€šé“åŸå›¾ + 16é€šé“ç™½è‰²æ§åˆ¶å›¾ = 32é€šé“
            if latent_model_input.shape[1] == 16:
                # æ­£å¸¸æµç¨‹ï¼šæ·»åŠ ç™½è‰²æ§åˆ¶latent
                white_control_latent = torch.ones_like(latent_model_input) * 0.5  # ç™½è‰²åœ¨latentç©ºé—´çš„è¿‘ä¼¼å€¼
                latent_model_input = torch.cat([latent_model_input, white_control_latent], dim=1)
                # æ›´æ–°é€šé“æ•°
                c = latent_model_input.shape[1]
            
            # ç¡®ä¿é«˜åº¦å’Œå®½åº¦æ˜¯2çš„å€æ•°ï¼ˆç”¨äºpatchifyï¼‰
            if h % 2 != 0 or w % 2 != 0:
                pad_h = (2 - h % 2) % 2
                pad_w = (2 - w % 2) % 2
                latent_model_input = F.pad(latent_model_input, (0, pad_w, 0, pad_h), mode='replicate')
                bs, c, h, w = latent_model_input.shape
            
            # Patchify: å°†latentè½¬æ¢ä¸ºpatch tokens
            # FLUXä½¿ç”¨2x2çš„patch
            latent_model_input_packed = latent_model_input.unfold(2, 2, 2).unfold(3, 2, 2)
            latent_model_input_packed = latent_model_input_packed.contiguous().view(
                bs, c, h//2 * w//2, 2, 2
            )
            latent_model_input_packed = latent_model_input_packed.permute(0, 2, 1, 3, 4).contiguous().view(
                bs, h//2 * w//2, c * 4
            )
            
            # ä¸ºå›¾åƒpatchåˆ›å»ºä½ç½®ID
            img_ids = torch.zeros(h // 2, w // 2, 3, device=latent_model_input.device)
            img_ids[..., 1] = img_ids[..., 1] + torch.arange(h // 2, device=latent_model_input.device)[:, None]
            img_ids[..., 2] = img_ids[..., 2] + torch.arange(w // 2, device=latent_model_input.device)[None, :]
            img_ids = img_ids.repeat(bs, 1, 1, 1).view(bs, -1, 3)
            
            # æ–‡æœ¬ä½ç½®ID
            txt_ids = torch.zeros(
                bs, text_embeddings.text_embeds.shape[1], 3, device=latent_model_input.device
            )
            
            # Guidanceå¤„ç†
            if self.transformer.config.guidance_embeds:
                if isinstance(guidance_embedding_scale, list):
                    guidance = torch.tensor(guidance_embedding_scale, device=latent_model_input.device)
                else:
                    guidance = torch.tensor([guidance_embedding_scale], device=latent_model_input.device)
                    guidance = guidance.expand(latent_model_input.shape[0])
            else:
                guidance = None

        # è°ƒç”¨transformerè¿›è¡Œé¢„æµ‹
        if bypass_guidance_embedding:
            from toolkit.models.flux import bypass_flux_guidance
            bypass_flux_guidance(self.transformer)
        
        noise_pred = self.transformer(
            hidden_states=latent_model_input_packed,
            timestep=timestep,
            encoder_hidden_states=text_embeddings.text_embeds,
            pooled_projections=text_embeddings.pooled_embeds,
            txt_ids=txt_ids,
            img_ids=img_ids,
            guidance=guidance,
            return_dict=False
        )[0]
        
        if bypass_guidance_embedding:
            from toolkit.models.flux import restore_flux_guidance
            restore_flux_guidance(self.transformer)
        
        # Unpatchify: å°†patch tokensè½¬æ¢å›latentæ ¼å¼
        noise_pred = noise_pred.view(bs, h//2 * w//2, 16, 2, 2)  # åªè¾“å‡º16é€šé“ï¼ˆåŸå§‹å›¾åƒï¼‰
        noise_pred = noise_pred.permute(0, 2, 1, 3, 4).contiguous().view(bs, 16, h//2, w//2, 2, 2)
        noise_pred = noise_pred.permute(0, 1, 2, 4, 3, 5).contiguous().view(bs, 16, h, w)
        
        return noise_pred