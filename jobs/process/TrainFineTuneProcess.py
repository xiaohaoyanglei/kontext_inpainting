from collections import OrderedDict
from jobs import TrainJob
from jobs.process import BaseTrainProcess
import torch
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader
from diffusers import FluxTransformer2DModel, AutoencoderKL
from toolkit.data_loader import ImageDataset
from transformers import CLIPTextModelWithProjection, CLIPTokenizer
from toolkit.models.flux import add_model_gpu_splitter_to_flux
import os

class TrainFineTuneProcess(BaseTrainProcess):
    def __init__(self, process_id: int, job: TrainJob, config: OrderedDict):
        super().__init__(process_id, job, config)

    def run(self):
        # è®¾ç½®PyTorchæ˜¾å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        print("ğŸ”§ å¯ç”¨PyTorchæ˜¾å­˜ç‰‡æ®µåŒ–ä¼˜åŒ–")
        
        token = "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"
        device = self.config.get('device', 'cuda')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        
        # 1. åŠ è½½ VAE - å¦‚æœæœ‰åŒGPUï¼Œæ”¾åˆ°GPU 1
        if torch.cuda.device_count() > 1:
            vae_device = 'cuda:1'
            print("ğŸ“ VAE æ”¾ç½®åˆ° GPU 1ï¼Œä¸ºä¸»æ¨¡å‹é‡Šæ”¾ GPU 0 æ˜¾å­˜")
        else:
            vae_device = device

        vae = AutoencoderKL.from_pretrained(
            self.config['vae_path'], token=token
        ).eval().to(vae_device)

        # 2. æ•°æ®åŠ è½½å™¨
        dataset = ImageDataset(
            {'include_prompt': True, 'resolution': self.config.get('resolution', 512)},
            source_dir=self.config['source_image_dir'],
            target_dir=self.config['target_image_dir'],
            mask_dir=self.config['mask_dir']
        )
        train_loader = DataLoader(
            dataset, 
            batch_size=self.config.get('batch_size', 1),
            shuffle=True,
            num_workers=0  # å‡å°‘å¤šè¿›ç¨‹æ˜¾å­˜å ç”¨
        )

        # 3. å¼ºåˆ¶ in_channels=64, patch_size=1
        in_channels = 64
        patch_size = 1
        patch_dim = in_channels * patch_size * patch_size
        print(f"å¼ºåˆ¶ä½¿ç”¨ in_channels={in_channels}, patch_size={patch_size}, patch_dim={patch_dim}")
        
        # 4. åŠ è½½æ¨¡å‹
        model_path = self.config['model_path']
        transformer_path = model_path + "/transformer" if not model_path.endswith("/transformer") else model_path
        
        model = FluxTransformer2DModel.from_pretrained(
            transformer_path,
            subfolder="",
            in_channels=in_channels,
            token=token,
            ignore_mismatched_sizes=True,
            low_cpu_mem_usage=False,
            axes_dims_rope=[64, 64]  # æ¯ä¸ªè½´64ç»´ï¼Œæ€»å…±128ç»´åŒ¹é…attention head
        )
        
        # 5. è‡ªåŠ¨æ¨æ–­ hidden_size
        if hasattr(model.config, 'joint_attention_dim') or 'joint_attention_dim' in model.config:
            hidden_size = getattr(model.config, 'joint_attention_dim', None) or model.config.get('joint_attention_dim', None)
            print(f"ä½¿ç”¨ joint_attention_dim ä½œä¸º hidden_size: {hidden_size}")
        elif (hasattr(model.config, 'num_attention_heads') or 'num_attention_heads' in model.config) and (hasattr(model.config, 'attention_head_dim') or 'attention_head_dim' in model.config):
            num_heads = getattr(model.config, 'num_attention_heads', None) or model.config.get('num_attention_heads', None)
            head_dim = getattr(model.config, 'attention_head_dim', None) or model.config.get('attention_head_dim', None)
            hidden_size = num_heads * head_dim
            print(f"ä½¿ç”¨ num_attention_heads * attention_head_dim ä½œä¸º hidden_size: {hidden_size}")
        else:
            raise ValueError(f"æ— æ³•åœ¨ config ä¸­æ‰¾åˆ° hidden_size ç›¸å…³å­—æ®µï¼Œæ‰€æœ‰ key: {list(model.config.keys())}")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥æ¨¡å‹çš„å®é™… hidden_size
        try:
            # æ£€æŸ¥ç¬¬ä¸€ä¸ª transformer block çš„ norm å±‚
            first_block = model.transformer_blocks[0] if hasattr(model, 'transformer_blocks') else None
            if first_block and hasattr(first_block, 'norm1'):
                actual_hidden_size = first_block.norm1.norm.normalized_shape[0]
                print(f"æ£€æµ‹åˆ°æ¨¡å‹å®é™… hidden_size: {actual_hidden_size}")
                if actual_hidden_size != hidden_size:
                    print(f"è­¦å‘Šï¼šé…ç½®çš„ hidden_size ({hidden_size}) ä¸å®é™…ä¸ç¬¦ï¼Œä½¿ç”¨å®é™…å€¼ {actual_hidden_size}")
                    hidden_size = actual_hidden_size
        except Exception as e:
            print(f"æ— æ³•æ£€æµ‹æ¨¡å‹å®é™… hidden_size: {e}")
            
        model.x_embedder = torch.nn.Linear(patch_dim, hidden_size, bias=True).to(device)
        torch.nn.init.trunc_normal_(model.x_embedder.weight, std=0.02)
        model.to(device)
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å¤§å¹…å‡å°‘æ˜¾å­˜ä½¿ç”¨
        model.enable_gradient_checkpointing()
        print("å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨")
        
        # ğŸ”¥ åŒGPUç­–ç•¥ï¼šæ¨¡å‹åœ¨GPU0ï¼ŒVAEå’Œæ–‡æœ¬ç¼–ç å™¨åœ¨GPU1
        if torch.cuda.device_count() > 1:
            print(f"ğŸ”¥ æ£€æµ‹åˆ° {torch.cuda.device_count()} å¼ GPUï¼Œå¯åŠ¨åŒGPUæ¨¡å¼ï¼")
            print("ğŸ’ åŒGPUååŒä½œæˆ˜ï¼š160GBæ˜¾å­˜å…¨é¢é‡Šæ”¾ï¼")
            
            # è®¾ç½®ä¸»è®¾å¤‡ä¸ºcuda:0
            device = 'cuda:0'
            model = model.to(device)
            
            print("ğŸš€ åŒGPUæ¨¡å¼ï¼šä¸»æ¨¡å‹GPU0ï¼ŒVAE/æ–‡æœ¬ç¼–ç å™¨GPU1")
        else:
            print("âš ï¸  åªæ£€æµ‹åˆ°å•å¡")
            device = 'cuda:0'
            model = model.to(device)

        print(f"Flux æ¨¡å‹åŠ è½½å®Œæˆï¼Œpatch_size={patch_size}ï¼Œpatch_dim={patch_dim}, hidden_size={hidden_size}, in_channels={in_channels}")
        print(f"x_embedder.weight.shape: {model.x_embedder.weight.shape}")
        print(f"model.config keys: {list(model.config.keys())}")
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å° axes_dims_rope çš„å®é™…å€¼
        print(f"æ¨¡å‹çš„ axes_dims_rope: {model.config.get('axes_dims_rope', 'NOT_FOUND')}")
        print(f"pos_embed.axes_dim: {getattr(model.pos_embed, 'axes_dim', 'NOT_FOUND')}")

        # 6. åŠ è½½æ–‡æœ¬ç¼–ç å™¨ä¸åˆ†è¯å™¨ - å¦‚æœæœ‰åŒGPUï¼Œä¹Ÿæ”¾åˆ°GPU 1
        if torch.cuda.device_count() > 1:
            text_encoder_device = 'cuda:1'
            print("ğŸ“ æ–‡æœ¬ç¼–ç å™¨æ”¾ç½®åˆ° GPU 1ï¼Œè¿›ä¸€æ­¥é‡Šæ”¾ GPU 0 æ˜¾å­˜")
        else:
            text_encoder_device = device
            
        text_encoder = CLIPTextModelWithProjection.from_pretrained(
            self.config['text_encoder_path'], token=token
        ).eval().to(text_encoder_device)
        
        clip_tokenizer = CLIPTokenizer.from_pretrained(
            "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", token=token
        )
        
        # æ–‡æœ¬æŠ•å½±å±‚è®¾å¤‡è®¾ç½® - è·Ÿéšæ–‡æœ¬ç¼–ç å™¨
        proj_hidden = torch.nn.Linear(
            text_encoder.config.hidden_size,
            4096  # ä½¿ç”¨ joint_attention_dimï¼Œä¸æ˜¯å®é™…çš„ hidden_size
        ).to(text_encoder_device)
        pooled_projection_dim = model.config.get('pooled_projection_dim', 768)
        proj_pooled = torch.nn.Linear(
            text_encoder.config.projection_dim,
            pooled_projection_dim
        ).to(text_encoder_device)
        
        # æ­£ç¡®åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡ï¼Œé˜²æ­¢NaN
        print("ğŸ”§ åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡...")
        torch.nn.init.xavier_uniform_(proj_hidden.weight)
        torch.nn.init.zeros_(proj_hidden.bias)
        torch.nn.init.xavier_uniform_(proj_pooled.weight)
        torch.nn.init.zeros_(proj_pooled.bias)
        print("âœ… æŠ•å½±å±‚æƒé‡åˆå§‹åŒ–å®Œæˆ")

        # 7. ä¼˜åŒ–å™¨
        try:
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit(
                list(model.parameters()) + list(proj_hidden.parameters()) + list(proj_pooled.parameters()),
                lr=self.config.get('lr', 1e-4)
            )
            print("ä½¿ç”¨ AdamW8bit ä¼˜åŒ–å™¨ä»¥èŠ‚çœæ˜¾å­˜")
        except ImportError:
            optimizer = AdamW(
                list(model.parameters()) + list(proj_hidden.parameters()) + list(proj_pooled.parameters()),
                lr=self.config.get('lr', 1e-4),
                eps=1e-8,
                weight_decay=0.01
            )
            print("ä½¿ç”¨æ ‡å‡† AdamW ä¼˜åŒ–å™¨")

        # 8. æ··åˆç²¾åº¦è®­ç»ƒ
        from torch.cuda.amp import autocast, GradScaler
        scaler = GradScaler()
        print("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨")
        
        # 9. è®­ç»ƒä¸»å¾ªç¯
        num_epochs = self.config.get('num_epochs', 1)
        
        for epoch in range(num_epochs):
            for step, batch in enumerate(train_loader):
                # æ‰¹æ¬¡æ•°æ®ç§»è‡³ GPU
                for k, v in batch.items():
                    if isinstance(v, torch.Tensor):
                        batch[k] = v.to(device)

                optimizer.zero_grad()
                
                # VAE ç¼–ç 
                with torch.no_grad():
                    # å¦‚æœVAEåœ¨GPU 1ï¼Œéœ€è¦ç§»åŠ¨æ•°æ®
                    if torch.cuda.device_count() > 1:
                        masked_img = batch['masked_image'].to('cuda:1')
                        target_img = batch['image'].to('cuda:1')
                        
                        latent_masked = vae.encode(masked_img).latent_dist.sample()
                        latent_target = vae.encode(target_img).latent_dist.sample()
                        
                        # ç¼–ç åç§»å›GPU 0
                        latent_masked = latent_masked.to('cuda:0')
                        latent_target = latent_target.to('cuda:0')
                        
                        del masked_img, target_img  # æ¸…ç†GPU 1ä¸Šçš„æ•°æ®
                    else:
                        latent_masked = vae.encode(batch['masked_image']).latent_dist.sample()
                        latent_target = vae.encode(batch['image']).latent_dist.sample()
                    torch.cuda.empty_cache()  # æ¸…ç† VAE ç¼–ç åçš„æ˜¾å­˜
                    
                noise = torch.randn_like(latent_masked)
                mask = batch['mask']
                print(f"åŸå§‹mask shape: {mask.shape}")
                if mask.shape[1] > 1:
                    print(f"è­¦å‘Šï¼šmaské€šé“æ•°ä¸º{mask.shape[1]}ï¼Œå°†åªå–ç¬¬ä¸€ä¸ªé€šé“")
                    mask = mask[:, :1, :, :]
                mask = F.interpolate(mask, size=latent_masked.shape[-2:], mode='nearest')
                print(f"å¤„ç†åmask shape: {mask.shape}")
                
                # åªå–å‰16é€šé“ latent_masked å’Œ noise
                latent_masked = latent_masked[:, :16, :, :]
                noise = noise[:, :16, :, :]
                # mask ä»ä¸º1é€šé“
                model_input = torch.cat([latent_masked, noise, mask], dim=1)  # [B, 33, H, W]
                
                # è‡ªåŠ¨è¡¥é›¶åˆ°in_channels=64
                if model_input.shape[1] < in_channels:
                    pad = torch.zeros((model_input.shape[0], in_channels - model_input.shape[1], model_input.shape[2], model_input.shape[3]), device=model_input.device, dtype=model_input.dtype)
                    model_input = torch.cat([model_input, pad], dim=1)
                elif model_input.shape[1] > in_channels:
                    model_input = model_input[:, :in_channels, :, :]
                print(f"æœ€ç»ˆé€å…¥æ¨¡å‹çš„shape: {model_input.shape}")
                
                # patchify æ£€æŸ¥
                B, C, H, W = model_input.shape
                patches = model_input.unfold(2, 1, 1).unfold(3, 1, 1)
                patches = patches.contiguous().view(B, C, -1, 1, 1)
                patches = patches.permute(0, 2, 1, 3, 4).reshape(-1, C * 1 * 1)
                print(f"patchify åçš„ shape: {patches.shape}")
                if H % patch_size != 0 or W % patch_size != 0:
                    raise ValueError(f"H/Wå¿…é¡»èƒ½è¢«patch_sizeæ•´é™¤ï¼Œå½“å‰H={H}, W={W}, patch_size={patch_size}")
                if C != in_channels:
                    raise ValueError(f"è¾“å…¥é€šé“æ•°åº”ä¸º{in_channels}ï¼Œå®é™…ä¸º{C}")

                # æ–‡æœ¬ç¼–ç  + æŠ•å°„
                with torch.no_grad():
                    # ç¡®å®šæ–‡æœ¬ç¼–ç è®¾å¤‡
                    if torch.cuda.device_count() > 1:
                        text_device = 'cuda:1'
                        clip_ids = clip_tokenizer(
                            batch['caption'], padding='max_length',
                            max_length=clip_tokenizer.model_max_length,
                            truncation=True, return_tensors='pt'
                        ).input_ids.to(text_device)
                        clip_out = text_encoder(
                            clip_ids, output_hidden_states=True, return_dict=True
                        )
                        hidden_states = clip_out.last_hidden_state
                        pooled_state  = clip_out.text_embeds
                        enc_states = proj_hidden(hidden_states)
                        pool_proj  = proj_pooled(pooled_state)
                        
                        # ç§»å›GPU 0ç”¨äºæ¨¡å‹è®¡ç®—
                        enc_states = enc_states.to('cuda:0')
                        pool_proj = pool_proj.to('cuda:0')
                    else:
                        clip_ids = clip_tokenizer(
                            batch['caption'], padding='max_length',
                            max_length=clip_tokenizer.model_max_length,
                            truncation=True, return_tensors='pt'
                        ).input_ids.to(device)
                        clip_out = text_encoder(
                            clip_ids, output_hidden_states=True, return_dict=True
                        )
                        hidden_states = clip_out.last_hidden_state
                        pooled_state  = clip_out.text_embeds
                        enc_states = proj_hidden(hidden_states)
                        pool_proj  = proj_pooled(pooled_state)
                    torch.cuda.empty_cache()  # æ¸…ç†æ–‡æœ¬ç¼–ç åçš„æ˜¾å­˜
                    
                print("pool_proj shape:", pool_proj.shape)  # åº”è¯¥æ˜¯ [B, 768]
                print(f"æ–‡æœ¬ç¼–ç  hidden_states shape: {hidden_states.shape}")
                print(f"æŠ•å½±å enc_states shape: {enc_states.shape}")
                txt_actual_seq_len = enc_states.shape[1]  # å®é™…æ–‡æœ¬åºåˆ—é•¿åº¦
                print(f"å®é™…æ–‡æœ¬åºåˆ—é•¿åº¦: {txt_actual_seq_len}")

                # é€å…¥æ¨¡å‹å‰ permute å¹¶ reshape ä¸ºåºåˆ—æ ¼å¼
                model_input_permuted = model_input.permute(0, 2, 3, 1).contiguous()  # [B, H, W, C]
                model_input_seq = model_input_permuted.reshape(model_input_permuted.shape[0], -1, model_input_permuted.shape[-1])  # [B, H*W, C]
                print(f"é€å…¥ model å‰çš„ shape: {model_input_seq.shape}")

                # æ„é€  img_idsï¼Œtxt_ids
                B, C, H, W = model_input.shape  # B=batch_size, H=64, W=64
                
                # txt_ids: å¯¹åº”æ–‡æœ¬åºåˆ—é•¿åº¦ï¼Œä½¿ç”¨ç®€å•çš„ä½ç½®ç¼–ç 
                txt_ids = torch.zeros(B, txt_actual_seq_len, 2, device=device, dtype=torch.long)
                for b in range(B):
                    for i in range(txt_actual_seq_len):
                        txt_ids[b, i] = torch.tensor([i // W, i % W], device=device)  # æ¨¡æ‹Ÿ 2D ä½ç½®
                
                # img_ids: å¯¹åº”å›¾åƒåºåˆ—é•¿åº¦ï¼Œä½¿ç”¨çœŸå®çš„ 2D ç½‘æ ¼åæ ‡
                img_seq_len = H * W  # 4096 for 64x64
                y_coords = torch.arange(H, device=device).unsqueeze(1).repeat(1, W).flatten()  # [4096]
                x_coords = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1).flatten()  # [4096]
                img_ids_single = torch.stack([y_coords, x_coords], dim=1)  # [4096, 2]
                img_ids = img_ids_single.unsqueeze(0).expand(B, -1, -1)  # [B, 4096, 2]
                
                print(f"ä¼ å…¥çš„ txt_ids shape: {txt_ids.shape}")  # [B, 77, 2]
                print(f"ä¼ å…¥çš„ img_ids shape: {img_ids.shape}")  # [B, 4096, 2]
                print(f"æ¨¡æ‹Ÿ cat åçš„ shape: {torch.cat([txt_ids, img_ids], dim=1).shape}")  # [B, 4173, 2]
                print(f"ids æœ€åä¸€ç»´ (n_axes): {txt_ids.shape[-1]}")
                total_seq_len = txt_ids.shape[1] + img_ids.shape[1]
                print(f"æ€»åºåˆ—é•¿åº¦: {total_seq_len} (txt: {txt_ids.shape[1]} + img: {img_ids.shape[1]})")

                # Flux æ¨¡å‹å‰å‘
                timesteps = torch.zeros(model_input.shape[0], dtype=torch.float32, device=device)  # [B] float32
                guidance = timesteps.clone()  # [B] float32
                
                print("ğŸ” æ£€æŸ¥è¾“å…¥æ•°æ®...")
                
                with autocast():
                    pred = model(
                        hidden_states=model_input_seq,
                        encoder_hidden_states=enc_states,
                        pooled_projections=pool_proj,
                        guidance=guidance,
                        timestep=timesteps,
                        txt_ids=txt_ids, # ä¼ å…¥ [B, 77, 2]
                        img_ids=img_ids, # ä¼ å…¥ [B, 4096, 2]
                        return_dict=False
                    )[0]

                print(f"æ¨¡å‹è¾“å‡º pred shape: {pred.shape}")
                print(f"ç›®æ ‡ latent_target shape: {latent_target.shape}")

                # æ¨¡å‹è¾“å‡ºç›´æ¥å°±æ˜¯å›¾åƒéƒ¨åˆ†
                img_pred = pred  # [B, img_seq_len, out_channels] = [1, 4096, 64]
                print(f"å›¾åƒè¾“å‡º img_pred shape: {img_pred.shape}")

                # é‡æ–°æ•´å½¢ä¸ºå›¾åƒæ ¼å¼ [B, C, H, W]
                B = img_pred.shape[0]  # 1
                img_seq_len = img_pred.shape[1]  # 4096
                out_channels = img_pred.shape[2]  # 64
                H = W = int(img_seq_len ** 0.5)  # sqrt(4096) = 64
                
                img_pred_reshaped = img_pred.permute(0, 2, 1).reshape(B, out_channels, H, W)
                print(f"é‡æ–°æ•´å½¢å img_pred_reshaped shape: {img_pred_reshaped.shape}")

                # åªå–å‰16ä¸ªé€šé“æ¥åŒ¹é…ç›®æ ‡ï¼ˆå› ä¸ºç›®æ ‡æ˜¯16é€šé“ï¼‰
                img_pred_matched = img_pred_reshaped[:, :16, :, :]  # [1, 16, 64, 64]
                print(f"é€šé“åŒ¹é…å img_pred_matched shape: {img_pred_matched.shape}")

                # æ¢¯åº¦ç´¯ç§¯
                gradient_accumulation_steps = 16 if torch.cuda.device_count() > 1 else 8
                print(f"ğŸ”¥ ä½¿ç”¨{gradient_accumulation_steps}æ­¥æ¢¯åº¦ç´¯ç§¯")
                
                with autocast():
                    loss = F.mse_loss(img_pred_matched, latent_target, reduction='mean')
                    loss = loss / gradient_accumulation_steps
                    print(f"âœ… æŸå¤±è®¡ç®—æ­£å¸¸: {loss.item():.6f}")

                print(f"ğŸ‰ å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—æˆåŠŸï¼Loss: {loss.item():.6f}")
                print("å¼€å§‹åå‘ä¼ æ’­...")
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()

                # åªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ‰æ›´æ–°ä¼˜åŒ–å™¨
                if (step + 1) % gradient_accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    torch.cuda.empty_cache()

                if step % 10 == 0:
                    print(f"Epoch {epoch} Step {step} Loss: {loss.item() * gradient_accumulation_steps:.4f}")
                    
                if step >= 19:  # æµ‹è¯•å‰20æ­¥
                    print("âœ… æµ‹è¯•å®Œæˆï¼å‰20æ­¥è®­ç»ƒæˆåŠŸ")
                    break
                    
        print("è®­ç»ƒå®Œæˆ")