from collections import OrderedDict
from jobs import TrainJob
from jobs.process import BaseTrainProcess
import torch
import torch.nn.functional as F
from torch.optim import AdamW
from torch.utils.data import DataLoader
from diffusers import FluxTransformer2DModel, AutoencoderKL
from toolkit.data_loader import ImageDataset, WhiteMaskDataset
from transformers import CLIPTextModelWithProjection, CLIPTokenizer, T5EncoderModel, T5TokenizerFast
from toolkit.models.flux import add_model_gpu_splitter_to_flux
from PIL import Image
from torchvision import transforms
import os
import psutil

# å°è¯•å¯¼å…¥ AdamW8bitï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ AdamW
try:
    from bitsandbytes.optim import AdamW8bit
    OPTIMIZER_CLASS = AdamW8bit
except ImportError:
    OPTIMIZER_CLASS = AdamW

class TrainFineTuneProcess(BaseTrainProcess):
    def __init__(self, process_id: int, job: TrainJob, config: OrderedDict):
        super().__init__(process_id, job, config)
        
        # Kontext-inpaint ä¸¤é˜¶æ®µè®­ç»ƒå‚æ•°
        self.two_stage_training = config.get('two_stage_training', True)
        self.stage1_steps = config.get('stage1_steps', 2000)
        self.stage1_lr = config.get('stage1_lr', 1e-4)
        self.stage2_lr = config.get('stage2_lr', 5e-5)
        self.current_step = 0
        self.is_stage1 = True
    
    def get_gpu_memory_info(self):
        """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024**3    # GB
            memory_free = torch.cuda.get_device_properties(0).total_memory / 1024**3 - memory_reserved  # GB
            return {
                'allocated': memory_allocated,
                'reserved': memory_reserved,
                'free': memory_free,
                'total': torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        return None
    
    def print_memory_status(self, step=None):
        """æ‰“å°å†…å­˜çŠ¶æ€"""
        gpu_info = self.get_gpu_memory_info()
        if gpu_info:
            step_info = f" [Step {step}]" if step is not None else ""
            print(f"ğŸ’¾ GPUæ˜¾å­˜{step_info}: "
                  f"å·²åˆ†é… {gpu_info['allocated']:.2f}GB, "
                  f"å·²ä¿ç•™ {gpu_info['reserved']:.2f}GB, "
                  f"å¯ç”¨ {gpu_info['free']:.2f}GB, "
                  f"æ€»è®¡ {gpu_info['total']:.2f}GB")
        
        # ç³»ç»Ÿå†…å­˜
        memory = psutil.virtual_memory()
        print(f"ğŸ’» ç³»ç»Ÿå†…å­˜: "
              f"å·²ç”¨ {memory.used / 1024**3:.2f}GB, "
              f"å¯ç”¨ {memory.available / 1024**3:.2f}GB, "
              f"æ€»è®¡ {memory.total / 1024**3:.2f}GB")

    def run(self):
        # è®¾ç½®PyTorchæ˜¾å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
        print("ğŸ”§ å¯ç”¨PyTorchæ˜¾å­˜ç‰‡æ®µåŒ–ä¼˜åŒ–")
        
        token = "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"
        device = self.config.get('device', 'cuda')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
        
        # 1. åŠ è½½ VAE - å¦‚æœæœ‰åŒGPUï¼Œæ”¾åˆ°GPU 1
        if torch.cuda.device_count() > 1:
            vae_device = 'cuda:1'
            print("ğŸ“ VAE æ”¾ç½®åˆ° GPU 1ï¼Œä¸ºä¸»æ¨¡å‹é‡Šæ”¾ GPU 0 æ˜¾å­˜")
        else:
            vae_device = device

        vae = AutoencoderKL.from_pretrained(
            self.config['vae_path'], token=token
        ).eval().to(vae_device)

        # 2. æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨ WhiteMaskDatasetï¼Œä½†æ·»åŠ latentç¼“å­˜
        from toolkit.data_loader import WhiteMaskDataset
        
        # åˆ›å»ºæ”¯æŒlatentç¼“å­˜çš„WhiteMaskDataset
        class CachedWhiteMaskDataset(WhiteMaskDataset):
            def __init__(self, config, source_dir=None, target_dir=None, mask_dir=None, vae=None):
                super().__init__(config, source_dir, target_dir, mask_dir)
                self.vae = vae
                self.latent_cache = {}
                self.disk_cache = {}
                
            def get_latent_cache_path(self, img_path):
                """è·å–latentç¼“å­˜è·¯å¾„"""
                img_dir = os.path.dirname(img_path)
                latent_dir = os.path.join(img_dir, '_latent_cache')
                filename_no_ext = os.path.splitext(os.path.basename(img_path))[0]
                # ç®€å•çš„ç¼“å­˜æ–‡ä»¶å
                cache_path = os.path.join(latent_dir, f'{filename_no_ext}.safetensors')
                return cache_path
                
            def load_or_encode_latent(self, img_tensor, cache_key):
                """åŠ è½½æˆ–ç¼–ç latent"""
                if cache_key in self.latent_cache:
                    return self.latent_cache[cache_key]
                
                # ç¼–ç latent
                with torch.no_grad():
                    if self.vae is not None:
                        # ç¡®ä¿è¾“å…¥å½¢çŠ¶æ­£ç¡® [B, C, H, W]
                        if img_tensor.dim() == 3:
                            img_tensor = img_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
                        elif img_tensor.dim() != 4:
                            raise ValueError(f"Expected 3D or 4D tensor, got {img_tensor.dim()}D")
                        
                        # å°†å›¾åƒtensorç§»åŠ¨åˆ°VAEæ‰€åœ¨çš„è®¾å¤‡
                        img_tensor = img_tensor.to(self.vae.device)
                        
                        latent = self.vae.encode(img_tensor).latent_dist.sample()
                        if latent.dim() == 4 and latent.shape[0] == 1:
                            latent = latent.squeeze(0)  # ç§»é™¤batchç»´åº¦
                    else:
                        # å¦‚æœæ²¡æœ‰VAEï¼Œè¿”å›éšæœºlatentï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
                        latent = torch.randn(16, 32, 32, device=img_tensor.device, dtype=img_tensor.dtype)
                
                # ç¼“å­˜åˆ°å†…å­˜
                self.latent_cache[cache_key] = latent
                return latent
            
            def pre_cache_all_latents(self):
                """é¢„ç¼“å­˜æ‰€æœ‰latentåˆ°ç£ç›˜"""
                print("ğŸ”„ å¼€å§‹é¢„ç¼“å­˜æ‰€æœ‰latent...")
                import os
                from safetensors.torch import save_file, load_file
                
                cached_count = 0
                total_count = len(self.file_list)
                
                for i, img_path in enumerate(self.file_list):
                    if i % 10 == 0:
                        print(f"ç¼“å­˜è¿›åº¦: {i}/{total_count}")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»ç¼“å­˜
                    cache_path = self.get_latent_cache_path(img_path)
                    if os.path.exists(cache_path):
                        cached_count += 1
                        continue
                    
                    try:
                        # åŠ è½½å›¾åƒ
                        img = Image.open(img_path)
                        img = img.convert('RGB')
                        img = img.resize((self.resolution, self.resolution), Image.BICUBIC)
                        img_tensor = transforms.ToTensor()(img)
                        
                        # å°†å›¾åƒtensorç§»åŠ¨åˆ°VAEæ‰€åœ¨çš„è®¾å¤‡
                        if self.vae is not None:
                            img_tensor = img_tensor.to(self.vae.device)
                        
                        # ç¼–ç latent
                        with torch.no_grad():
                            if img_tensor.dim() == 3:
                                img_tensor = img_tensor.unsqueeze(0)
                            latent = self.vae.encode(img_tensor).latent_dist.sample()
                            if latent.dim() == 4 and latent.shape[0] == 1:
                                latent = latent.squeeze(0)
                        
                        # ä¿å­˜åˆ°ç£ç›˜
                        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
                        save_file({'latent': latent.cpu()}, cache_path)
                        cached_count += 1
                        
                    except Exception as e:
                        print(f"ç¼“å­˜å¤±è´¥ {img_path}: {e}")
                
                print(f"âœ… é¢„ç¼“å­˜å®Œæˆ: {cached_count}/{total_count} ä¸ªlatentå·²ç¼“å­˜")
            
            def load_cached_latent(self, img_path):
                """ä»ç£ç›˜åŠ è½½ç¼“å­˜çš„latent"""
                cache_path = self.get_latent_cache_path(img_path)
                if os.path.exists(cache_path):
                    try:
                        from safetensors.torch import load_file
                        state_dict = load_file(cache_path, device='cpu')
                        return state_dict['latent']
                    except Exception as e:
                        print(f"åŠ è½½ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
                        return None
                return None
        
        dataset = CachedWhiteMaskDataset(
            config={'include_prompt': True, 'resolution': self.config.get('resolution', 256)},
            source_dir=self.config['source_image_dir'],
            target_dir=self.config['target_image_dir'],
            vae=vae  # ä¼ å…¥VAEç”¨äºç¼–ç 
        )
        
        # é¢„ç¼“å­˜æ‰€æœ‰latent
        print("ğŸ”„ æ£€æŸ¥å¹¶é¢„ç¼“å­˜latent...")
        dataset.pre_cache_all_latents()
        
        train_loader = DataLoader(
            dataset, 
            batch_size=self.config.get('batch_size', 1),
            shuffle=True,
            num_workers=0  # å‡å°‘å¤šè¿›ç¨‹æ˜¾å­˜å ç”¨
        )

        # 3. å¼ºåˆ¶ in_channels=64, patch_size=1 (æ”¯æŒ32é€šé“è¾“å…¥)
        in_channels = 64
        patch_size = 1
        patch_dim = in_channels * patch_size * patch_size
        print(f"å¼ºåˆ¶ä½¿ç”¨ in_channels={in_channels}, patch_size={patch_size}, patch_dim={patch_dim}")
        print(f"ğŸ¯ Kontext-inpaint: 32é€šé“è¾“å…¥ (åŸå›¾16 + çº¯ç™½æ§åˆ¶å›¾16)")
        
        # 4. åŠ è½½æ¨¡å‹
        model_path = self.config['model_path']
        transformer_path = model_path + "/transformer" if not model_path.endswith("/transformer") else model_path
        
        model = FluxTransformer2DModel.from_pretrained(
            transformer_path,
            subfolder="",
            in_channels=in_channels,
            token=token,
            ignore_mismatched_sizes=True,
            low_cpu_mem_usage=False,
            axes_dims_rope=[64, 64]  # æ¯ä¸ªè½´64ç»´ï¼Œæ€»å…±128ç»´åŒ¹é…attention head
        )
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®
        print(f"ä½¿ç”¨ joint_attention_dim ä½œä¸º hidden_size: {model.config.joint_attention_dim}")
        actual_hidden_size = model.config.joint_attention_dim
        print(f"æ£€æµ‹åˆ°æ¨¡å‹å®é™… hidden_size: {actual_hidden_size}")

        # é¢å¤–è°ƒè¯•ï¼šå›¾åƒä¸æ–‡æœ¬é€šé“çš„éšè—ç»´åº¦æ˜¯å¦ä¸€è‡´
        try:
            x_embedder_out = int(model.x_embedder.weight.shape[0])
            x_embedder_in = int(model.x_embedder.weight.shape[1])
            print(f"x_embedder.in_features: {x_embedder_in}, x_embedder.out_features: {x_embedder_out}")
            if x_embedder_out != actual_hidden_size:
                print("âš ï¸ ç»´åº¦ä¸ä¸€è‡´ï¼šx_embedder.out_features != joint_attention_dimï¼Œæ¨¡å‹å†…éƒ¨ä¼šåšé€‚é…ï¼Œä½†å¯èƒ½æ•°å€¼æ›´æ•æ„Ÿ")
        except Exception as _:
            pass
        
        # ä¸ºç¡®ä¿æ•°å€¼ç¨³å®šï¼Œå¼ºåˆ¶é‡ç½® x_embedderï¼ˆä¸ç¨³å®šç‰ˆæœ¬ä¸€è‡´ï¼‰
        try:
            torch.nn.init.xavier_uniform_(model.x_embedder.weight)
            if hasattr(model.x_embedder, 'bias') and model.x_embedder.bias is not None:
                torch.nn.init.zeros_(model.x_embedder.bias)
            print("ğŸ”§ å·²å¼ºåˆ¶é‡ç½® x_embedder æƒé‡ (Xavier) â€” å›åˆ°ç¨³å®šç‰ˆæœ¬ç­–ç•¥")
        except Exception as _:
            pass

        # ä¸¤é˜¶æ®µè®­ç»ƒè®¾ç½®
        if self.two_stage_training:
            print("ğŸ”„ å¯ç”¨ä¸¤é˜¶æ®µè®­ç»ƒ:")
            print(f"   - ç¬¬ä¸€é˜¶æ®µ: åªè®­ç»ƒ x_embedder (æ­¥æ•°: {self.stage1_steps})")
            print("   - ç¬¬äºŒé˜¶æ®µ: å…¨æ¨¡å‹å¾®è°ƒ")
            print("â„ï¸  ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“å…¨æ¨¡å‹ï¼Œåªè®­ç»ƒ x_embedder")
            self._freeze_all_except_projection(model)
        
        # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥é™ä½æ˜¾å­˜å³°å€¼
        try:
            model.enable_gradient_checkpointing()
            print("âš™ï¸ å·²å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient checkpointing)")
        except Exception as _:
            print("âš ï¸ å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹å¤±è´¥ï¼Œç»§ç»­ä»¥é»˜è®¤æ–¹å¼è®­ç»ƒ")
        
        # ğŸ”¥ åŒGPUç­–ç•¥ï¼šæ¨¡å‹åœ¨GPU0ï¼ŒVAEå’Œæ–‡æœ¬ç¼–ç å™¨åœ¨GPU1
        if torch.cuda.device_count() > 1:
            print(f"ğŸ”¥ æ£€æµ‹åˆ° {torch.cuda.device_count()} å¼ GPUï¼Œå¯åŠ¨åŒGPUæ¨¡å¼ï¼")
            print("ğŸ’ åŒGPUååŒä½œæˆ˜ï¼š160GBæ˜¾å­˜å…¨é¢é‡Šæ”¾ï¼")
            
            # è®¾ç½®ä¸»è®¾å¤‡ä¸ºcuda:0
            device = 'cuda:0'
            model = model.to(device)
            
            print("ğŸš€ åŒGPUæ¨¡å¼ï¼šä¸»æ¨¡å‹GPU0ï¼ŒVAE/æ–‡æœ¬ç¼–ç å™¨GPU1")
        else:
            print("âš ï¸  åªæ£€æµ‹åˆ°å•å¡")
            device = 'cuda:0'
            model = model.to(device)

        print(f"Flux æ¨¡å‹åŠ è½½å®Œæˆï¼Œpatch_size={patch_size}ï¼Œpatch_dim={patch_dim}, hidden_size={actual_hidden_size}, in_channels={in_channels}")
        print(f"x_embedder.weight.shape: {model.x_embedder.weight.shape}")
        print(f"model.config keys: {list(model.config.keys())}")
        print(f"æ¨¡å‹çš„ axes_dims_rope: {model.config.axes_dims_rope}")
        print(f"pos_embed.axes_dim: {model.config.axes_dims_rope}")
        
        # 6. åŠ è½½æ–‡æœ¬ç¼–ç å™¨ä¸åˆ†è¯å™¨ - å¦‚æœæœ‰åŒGPUï¼Œä¹Ÿæ”¾åˆ°GPU 1
        # å°†æ–‡æœ¬ç¼–ç å™¨æ”¾ç½®åˆ° CPUï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨
        text_encoder_device = 'cpu'
        print("ğŸ“ æ–‡æœ¬ç¼–ç å™¨æ”¾ç½®åˆ° CPUï¼Œé™ä½æ˜¾å­˜å³°å€¼")
            
        # åŠ è½½ç¬¬ä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ (CLIP)
        text_encoder = CLIPTextModelWithProjection.from_pretrained(
            self.config['text_encoder_path'], token=token
        ).eval().to(text_encoder_device)
        
        # åŠ è½½ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨ (T5)
        text_encoder_2 = T5EncoderModel.from_pretrained(
            self.config['text_encoder_2_path'], token=token
        ).eval().to(text_encoder_device)
        
        # åŠ è½½å¯¹åº”çš„tokenizer
        clip_tokenizer = CLIPTokenizer.from_pretrained(
            "/cloud/cloud-ssd1/FLUX.1-Fill-dev/tokenizer", token=token
        )
        
        t5_tokenizer = T5TokenizerFast.from_pretrained(
            "/cloud/cloud-ssd1/FLUX.1-Fill-dev/tokenizer_2", token=token
        )
        
        # æ–‡æœ¬æŠ•å½±å±‚æ”¾åœ¨ä¸»è®¾å¤‡ï¼ˆGPUï¼‰ï¼Œä»¥é¿å…ä¼˜åŒ–å™¨/8bitåœ¨CPUä¸Šçš„è®¾å¤‡ä¸ä¸€è‡´é—®é¢˜
        # ç»´åº¦æŒ‰æ¨¡å‹ joint_attention_dimï¼ˆå¦‚ 4096ï¼‰
        joint_dim = int(model.config.joint_attention_dim)
        proj_hidden = torch.nn.Linear(
            text_encoder.config.hidden_size,
            joint_dim
        ).to(device)
        pooled_projection_dim = model.config.get('pooled_projection_dim', 768)
        proj_pooled = torch.nn.Linear(
            text_encoder.config.projection_dim,
            pooled_projection_dim
        ).to(device)
        
        # æ­£ç¡®åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡ï¼Œé˜²æ­¢NaN
        print("ğŸ”§ åˆå§‹åŒ–æŠ•å½±å±‚æƒé‡...")
        torch.nn.init.xavier_uniform_(proj_hidden.weight)
        torch.nn.init.zeros_(proj_hidden.bias)
        torch.nn.init.xavier_uniform_(proj_pooled.weight)
        torch.nn.init.zeros_(proj_pooled.bias)
        # å°†æŠ•å½±å±‚ç¼©æ”¾ï¼Œé¿å…è¿‡å¤§æ•°å€¼è¿›å…¥ä¸»å¹²
        with torch.no_grad():
            proj_hidden.weight.mul_(0.02)
            proj_pooled.weight.mul_(0.02)
        print(f"âœ… æŠ•å½±å±‚æƒé‡åˆå§‹åŒ–å®Œæˆ (é™„åŠ ç¼©æ”¾ 0.02)ï¼Œproj_hidden.out_features={proj_hidden.out_features}, joint_dim={joint_dim}")

        # å¦‚ x_embedder æƒé‡å¼‚å¸¸ä¸ºå…¨é›¶ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡ç¨³å¥åˆå§‹åŒ–
        try:
            if float(model.x_embedder.weight.norm().item()) == 0.0:
                torch.nn.init.xavier_uniform_(model.x_embedder.weight)
                if hasattr(model.x_embedder, 'bias') and model.x_embedder.bias is not None:
                    torch.nn.init.zeros_(model.x_embedder.bias)
                with torch.no_grad():
                    model.x_embedder.weight.mul_(0.02)
                print("âš ï¸ æ£€æµ‹åˆ° x_embedder æƒé‡èŒƒæ•°ä¸º 0ï¼Œå·²æ‰§è¡Œ Xavier åˆå§‹åŒ–å¹¶ç¼©æ”¾ 0.02")
        except Exception:
            pass

        # 7. ä¼˜åŒ–å™¨ï¼ˆä¸º x_embedder è®¾ç½®æ›´ç¨³å¥çš„å­¦ä¹ ç‡ä¸æƒé‡è¡°å‡ï¼‰
        stage1_lr = self.config.get('stage1_lr', self.config.get('lr', 1e-4))
        # x_embedder ç”¨æ›´ä½ LRï¼ˆä¸Šé™ 5e-5ï¼‰ä¸æ›´å¼º WDï¼Œç¼“è§£æƒé‡èŒƒæ•°è†¨èƒ€
        x_embedder_lr = min(stage1_lr, 5e-5)
        x_embedder_wd = 0.05
        other_wd = 0.01
        x_embedder_params = list(model.x_embedder.parameters()) if hasattr(model, 'x_embedder') else []
        other_params = [p for p in model.parameters() if p.requires_grad and (id(p) not in {id(pp) for pp in x_embedder_params})]
        proj_params = list(proj_hidden.parameters()) + list(proj_pooled.parameters())

        try:
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit([
                {"params": x_embedder_params, "lr": x_embedder_lr, "weight_decay": x_embedder_wd},
                {"params": other_params, "lr": stage1_lr, "weight_decay": other_wd},
                {"params": proj_params, "lr": stage1_lr, "weight_decay": other_wd},
            ])
            print("ä½¿ç”¨ AdamW8bit ä¼˜åŒ–å™¨ä»¥èŠ‚çœæ˜¾å­˜ï¼ˆx_embedder å¯ç”¨æ›´ä½ LR/æ›´é«˜ WDï¼‰")
        except ImportError:
            optimizer = OPTIMIZER_CLASS([
                {"params": x_embedder_params, "lr": x_embedder_lr, "weight_decay": x_embedder_wd},
                {"params": other_params, "lr": stage1_lr, "weight_decay": other_wd},
                {"params": proj_params, "lr": stage1_lr, "weight_decay": other_wd},
            ])
            print(f"ä½¿ç”¨ {OPTIMIZER_CLASS.__name__} ä¼˜åŒ–å™¨ï¼ˆx_embedder å¯ç”¨æ›´ä½ LR/æ›´é«˜ WDï¼‰")

        # 8. æ··åˆç²¾åº¦è®­ç»ƒ
        from torch.cuda.amp import autocast, GradScaler
        scaler = GradScaler()
        print("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥å‡å°‘æ˜¾å­˜ä½¿ç”¨")
        
        # 9. è®­ç»ƒä¸»å¾ªç¯ï¼ˆä»¥ steps ä¸ºä¸»é©±åŠ¨ï¼Œepoch ä»…ç”¨äºè®¡æ•°ï¼‰
        num_epochs = self.config.get('num_epochs', 1)
        total_steps = self.config.get('steps', 10000)
        gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 4)
        
        print(f"å¼€å§‹è®­ç»ƒ: {num_epochs} epochs (ä»…ç”¨äºæ˜¾ç¤º), {total_steps} steps (ä¸»é©±åŠ¨), æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
        try:
            print(f"æ•°æ®é›†æ ·æœ¬æ•°ï¼ˆé…å¯¹ï¼‰: {len(dataset)}")
        except Exception:
            pass
        
        global_step = 0
        # æ–‡æœ¬ç¼–ç ç¼“å­˜ï¼šcaption -> (hidden_states_cpu, pooled_state_cpu)
        text_encode_cache = {}
        epoch = 0
        
        while global_step < total_steps:
            for step, batch in enumerate(train_loader):
                # æ›´æ–°è®­ç»ƒé˜¶æ®µ
                self.update_training_stage(global_step, model, optimizer)
                
                # æ‰¹æ¬¡æ•°æ®ç§»è‡³ GPU
                for k, v in batch.items():
                    if isinstance(v, torch.Tensor):
                        batch[k] = v.to(device)

                optimizer.zero_grad()
                
                # ä½¿ç”¨ç¼“å­˜çš„latent - é¿å…é‡å¤VAEç¼–ç 
                with torch.no_grad():
                    # ç”Ÿæˆç¼“å­˜é”®
                    source_key = f"source_{step}"
                    control_key = f"control_{step}"
                    target_key = f"target_{step}"
                    
                    # ä½¿ç”¨ç¼“å­˜çš„latentæˆ–é‡æ–°ç¼–ç 
                    if hasattr(dataset, 'load_or_encode_latent'):
                        # ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜
                        latent_source = dataset.load_or_encode_latent(batch['source_image'], source_key)
                        latent_control = dataset.load_or_encode_latent(batch['control_tensor'], control_key)
                        latent_target = dataset.load_or_encode_latent(batch['tensor'], target_key)
                    else:
                        # å›é€€åˆ°ç›´æ¥ç¼–ç 
                        if torch.cuda.device_count() > 1:
                            source_img = batch['source_image'].to('cuda:1')
                            control_img = batch['control_tensor'].to('cuda:1')
                            target_img = batch['tensor'].to('cuda:1')
                            
                            latent_source = vae.encode(source_img).latent_dist.sample()
                            latent_control = vae.encode(control_img).latent_dist.sample()
                            latent_target = vae.encode(target_img).latent_dist.sample()
                            
                            latent_source = latent_source.to('cuda:0')
                            latent_control = latent_control.to('cuda:0')
                            latent_target = latent_target.to('cuda:0')
                            
                            del source_img, control_img, target_img
                        else:
                            latent_source = vae.encode(batch['source_image']).latent_dist.sample()
                            latent_control = vae.encode(batch['control_tensor']).latent_dist.sample()
                            latent_target = vae.encode(batch['tensor']).latent_dist.sample()
                    
                    torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
                    
                # 32é€šé“è¾“å…¥ï¼šåŸå›¾16 + çº¯ç™½æ§åˆ¶å›¾16
                # æ£€æŸ¥latentç»´åº¦å¹¶ç¡®ä¿å½¢çŠ¶æ­£ç¡®
                if latent_source.dim() == 3:
                    # å¦‚æœæ˜¯3Dï¼Œæ·»åŠ batchç»´åº¦
                    latent_source = latent_source.unsqueeze(0)
                    latent_control = latent_control.unsqueeze(0)
                    latent_target = latent_target.unsqueeze(0)
                
                # åªå–å‰16é€šé“
                latent_source = latent_source[:, :16, :, :]  # åªå–å‰16é€šé“
                latent_control = latent_control[:, :16, :, :]  # åªå–å‰16é€šé“
                
                # æ‹¼æ¥ï¼šåŸå›¾latent(16) + æ§åˆ¶å›¾latent(16) = 32é€šé“
                model_input = torch.cat([latent_source, latent_control], dim=1)  # [B, 32, H, W]
                
                # è‡ªåŠ¨è¡¥é›¶åˆ°in_channels=64
                if model_input.shape[1] < in_channels:
                    pad = torch.zeros((model_input.shape[0], in_channels - model_input.shape[1], model_input.shape[2], model_input.shape[3]), device=model_input.device, dtype=model_input.dtype)
                    model_input = torch.cat([model_input, pad], dim=1)
                elif model_input.shape[1] > in_channels:
                    model_input = model_input[:, :in_channels, :, :]
                
                if global_step % 100 == 0:  # æ¯ç™¾æ­¥æ£€æŸ¥ä¸€æ¬¡
                    print(f"Step {global_step}: æœ€ç»ˆé€å…¥æ¨¡å‹çš„shape: {model_input.shape}")
                    try:
                        print(f"model_input stats -> min: {model_input.min().item():.4f}, max: {model_input.max().item():.4f}, mean: {model_input.mean().item():.4f}")
                    except Exception:
                        pass
                
                # patchify æ£€æŸ¥
                B, C, H, W = model_input.shape
                patches = model_input.unfold(2, 1, 1).unfold(3, 1, 1)
                patches = patches.contiguous().view(B, C, -1, 1, 1)
                patches = patches.permute(0, 2, 1, 3, 4).reshape(-1, C * 1 * 1)
                
                if H % patch_size != 0 or W % patch_size != 0:
                    raise ValueError(f"H/Wå¿…é¡»èƒ½è¢«patch_sizeæ•´é™¤ï¼Œå½“å‰H={H}, W={W}, patch_size={patch_size}")
                if C != in_channels:
                    raise ValueError(f"è¾“å…¥é€šé“æ•°åº”ä¸º{in_channels}ï¼Œå®é™…ä¸º{C}")

                # æ–‡æœ¬ç¼–ç ï¼ˆæ— æ¢¯åº¦ï¼‰+ æŠ•å½±ï¼ˆéœ€æ¢¯åº¦ï¼‰
                # æ–‡æœ¬ä¾§ç»Ÿä¸€åœ¨ CPU ç¼–ç 
                text_device = 'cpu'

                with torch.no_grad():
                    # ç»Ÿä¸€ç”Ÿæˆç¼“å­˜é”®ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
                    cap = batch['caption']
                    if isinstance(cap, (list, tuple)):
                        cap_key = "\n".join([str(x) for x in cap])
                    else:
                        cap_key = str(cap)
                    if cap_key in text_encode_cache:
                        hidden_states, pooled_state = text_encode_cache[cap_key]
                    else:
                        # CLIP ç¼–ç ï¼ˆCPUï¼‰
                        clip_ids = clip_tokenizer(
                            cap, padding='max_length',
                            max_length=clip_tokenizer.model_max_length,
                            truncation=True, return_tensors='pt'
                        ).input_ids.to(text_device)
                        clip_out = text_encoder(
                            clip_ids, output_hidden_states=True, return_dict=True
                        )
                        hidden_states = clip_out.last_hidden_state.cpu()  # ç¼“å­˜ä¸ºCPUå¼ é‡
                        pooled_state = clip_out.text_embeds.cpu()

                        # å¯é€‰ï¼šT5 ç¼–ç ï¼ˆCPUï¼‰ç›®å‰ä¸å‚ä¸æŠ•å½±ï¼Œè®¡ç®—æˆæœ¬å¤§ï¼Œé»˜è®¤è·³è¿‡
                        # å¦‚éœ€å¯ç”¨ï¼Œå–æ¶ˆä¸‹æ–¹æ³¨é‡Šå¹¶å¯åŠ å…¥ç¼“å­˜
                        # t5_ids = t5_tokenizer(
                        #     cap, padding='max_length',
                        #     max_length=t5_tokenizer.model_max_length,
                        #     truncation=True, return_tensors='pt'
                        # ).input_ids.to(text_device)
                        # t5_out = text_encoder_2(
                        #     t5_ids, output_hidden_states=True, return_dict=True
                        # )
                        # t5_hidden_states = t5_out.last_hidden_state

                        text_encode_cache[cap_key] = (hidden_states, pooled_state)

                # æŠ•å½±ï¼ˆå¯ç”¨æ¢¯åº¦ï¼›åœ¨GPUï¼‰
                enc_states = proj_hidden(hidden_states.to(device))
                pool_proj = proj_pooled(pooled_state.to(device))

                # enc_states / pool_proj å·²åœ¨ GPU

                # ä¸æ¨¡å‹è¾“å…¥dtypeå¯¹é½å¹¶è£å‰ªï¼Œæå‡æ•°å€¼ç¨³å®šæ€§
                enc_states = enc_states.to(dtype=model_input.dtype)
                pool_proj = pool_proj.to(dtype=model_input.dtype)
                enc_states = torch.clamp(enc_states, -5.0, 5.0)
                pool_proj = torch.clamp(pool_proj, -5.0, 5.0)
                
                if global_step % 100 == 0:
                    print(f"pool_proj shape: {pool_proj.shape}")
                    try:
                        print(f"pool_proj stats -> min: {pool_proj.min().item():.4f}, max: {pool_proj.max().item():.4f}, mean: {pool_proj.mean().item():.4f}")
                    except Exception:
                        pass
                    print(f"æ–‡æœ¬ç¼–ç  hidden_states shape: {hidden_states.shape}")
                    print(f"æŠ•å½±å enc_states shape: {enc_states.shape}")
                    try:
                        print(f"enc_states stats -> min: {enc_states.min().item():.4f}, max: {enc_states.max().item():.4f}, mean: {enc_states.mean().item():.4f}")
                    except Exception:
                        pass
                    try:
                        if 'x_embedder_out' in locals() and enc_states.shape[-1] != x_embedder_out:
                            print("âš ï¸ ç»´åº¦ä¸ä¸€è‡´ï¼šenc_statesæœ€åç»´åº¦ä¸x_embedder.out_featuresä¸åŒï¼Œå¯èƒ½é€ æˆèåˆä¸ç¨³å®š")
                    except Exception:
                        pass
                    print(f"å®é™…æ–‡æœ¬åºåˆ—é•¿åº¦: {hidden_states.shape[1]}")
                
                # æ¨¡å‹è¾“å…¥åºåˆ—åŒ–
                model_input_seq = patches  # [B*H*W, C*patch_size*patch_size]
                if global_step % 100 == 0:
                    try:
                        print(f"model_input_seq shape: {model_input_seq.shape}")
                    except Exception:
                        pass
                
                if global_step % 100 == 0:
                    print(f"é€å…¥ model å‰çš„ shape: {model_input_seq.shape}")
                
                # ä½ç½®ç¼–ç IDç”Ÿæˆ
                txt_ids = torch.zeros((B, hidden_states.shape[1], 2), device=device, dtype=torch.long)  # [B, 77, 2]
                y_coords = torch.arange(H, device=device).unsqueeze(1).repeat(1, W).flatten()  # [4096]
                x_coords = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1).flatten()  # [4096]
                img_ids_single = torch.stack([y_coords, x_coords], dim=1)  # [4096, 2]
                img_ids = img_ids_single.unsqueeze(0).expand(B, -1, -1)  # [B, 4096, 2]
                
                if global_step % 100 == 0:
                    print(f"ä¼ å…¥çš„ txt_ids shape: {txt_ids.shape}")  # [B, 77, 2]
                    print(f"ä¼ å…¥çš„ img_ids shape: {img_ids.shape}")  # [B, 4096, 2]
                    print(f"æ¨¡æ‹Ÿ cat åçš„ shape: {torch.cat([txt_ids, img_ids], dim=1).shape}")  # [B, 4173, 2]
                    print(f"ids æœ€åä¸€ç»´ (n_axes): {txt_ids.shape[-1]}")
                    total_seq_len = txt_ids.shape[1] + img_ids.shape[1]
                    print(f"æ€»åºåˆ—é•¿åº¦: {total_seq_len} (txt: {txt_ids.shape[1]} + img: {img_ids.shape[1]})")

                # Flux æ¨¡å‹å‰å‘
                timesteps = torch.zeros(model_input.shape[0], dtype=torch.float32, device=device)  # [B] float32
                guidance = timesteps.clone()  # [B] float32
                
                if global_step % 100 == 0:
                    print("ğŸ” æ£€æŸ¥è¾“å…¥æ•°æ®...")
                
                # æ¸…ç†æ½œåœ¨çš„éæœ‰é™å€¼ï¼ˆä¿å­˜åŸé€»è¾‘ï¼Œä½†é˜ˆå€¼æ”¶ç´§ä¸€ç‚¹ï¼‰
                model_input = torch.nan_to_num(model_input, nan=0.0, posinf=1e2, neginf=-1e2)
                # è¿›ä¸€æ­¥ç¨³å¥åŒ–ï¼šå¯¹å›¾åƒä¾§è¾“å…¥åšå¤¹æ–­ï¼Œé¿å…å¼‚å¸¸å¹…å€¼å¯¼è‡´æ•°å€¼çˆ†ç‚¸
                model_input = torch.clamp(model_input, -5.0, 5.0)
                enc_states = torch.nan_to_num(enc_states, nan=0.0, posinf=1e2, neginf=-1e2)
                pool_proj = torch.nan_to_num(pool_proj, nan=0.0, posinf=1e2, neginf=-1e2)

                with autocast(dtype=torch.bfloat16):
                    pred = model(
                        hidden_states=model_input_seq,
                        encoder_hidden_states=enc_states,
                        pooled_projections=pool_proj,
                        guidance=guidance,
                        timestep=timesteps,
                        txt_ids=txt_ids, # ä¼ å…¥ [B, 77, 2]
                        img_ids=img_ids, # ä¼ å…¥ [B, 4096, 2]
                        return_dict=False
                    )[0]

                if global_step % 100 == 0:
                    print(f"æ¨¡å‹è¾“å‡º pred shape: {pred.shape}")
                    print(f"ç›®æ ‡ latent_target shape: {latent_target.shape}")

                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if not torch.isfinite(pred).all():
                    print("âŒ æ£€æµ‹åˆ° pred å«æœ‰éæœ‰é™å€¼ (NaN/Inf)ï¼Œè·³è¿‡æ­¤æ­¥")
                    continue

                # æ¨¡å‹è¾“å‡ºç›´æ¥å°±æ˜¯å›¾åƒéƒ¨åˆ†
                img_pred = pred  # [B, img_seq_len, out_channels] = [1, 4096, 64]
                
                if global_step % 100 == 0:
                    print(f"å›¾åƒè¾“å‡º img_pred shape: {img_pred.shape}")

                # é‡æ–°æ•´å½¢ä¸ºå›¾åƒæ ¼å¼ [B, C, H, W]
                B = img_pred.shape[0]  # 1
                img_seq_len = img_pred.shape[1]  # 4096
                out_channels = img_pred.shape[2]  # 64
                H = W = int(img_seq_len ** 0.5)  # sqrt(4096) = 64
                
                img_pred_reshaped = img_pred.permute(0, 2, 1).reshape(B, out_channels, H, W)
                
                if global_step % 100 == 0:
                    print(f"é‡æ–°æ•´å½¢å img_pred_reshaped shape: {img_pred_reshaped.shape}")

                # åªå–å‰16ä¸ªé€šé“æ¥åŒ¹é…ç›®æ ‡ï¼ˆå› ä¸ºç›®æ ‡æ˜¯16é€šé“ï¼‰
                img_pred_matched = img_pred_reshaped[:, :16, :, :]  # [1, 16, 64, 64]
                
                if global_step % 100 == 0:
                    print(f"é€šé“åŒ¹é…å img_pred_matched shape: {img_pred_matched.shape}")
                
                # æ£€æŸ¥NaNå€¼
                if torch.isnan(img_pred_matched).any():
                    print("âŒ è­¦å‘Šï¼šæ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼ï¼")
                    continue
                
                if torch.isnan(latent_target).any():
                    print("âŒ è­¦å‘Šï¼šç›®æ ‡å€¼åŒ…å«NaNå€¼ï¼")
                    continue
                
                with autocast(dtype=torch.bfloat16):
                    # å·®å¼‚åŠ æƒ MSEï¼ˆæ¸©å’Œç‰ˆ + ç¬¬äºŒé˜¶æ®µçƒ­èº«ï¼‰ï¼š
                    # ç¬¬äºŒé˜¶æ®µåˆ‡æ¢åçš„å‰300æ­¥ï¼Œè¿›ä¸€æ­¥å‡å¼±æƒé‡å¼ºåº¦ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
                    with torch.no_grad():
                        delta = (latent_target - latent_source).abs()  # [B, 16, H, W]
                        delta = torch.clamp(delta - delta.mean(), -2.0, 2.0)
                        warmup = (not self.is_stage1) and (global_step < self.stage1_steps + 300)
                        base_scale = 1.0 if warmup else 2.0  # çƒ­èº«æœŸæ›´æ¸©å’Œ
                        slope = 2.0 if warmup else 3.0
                        weight = 1.0 + base_scale * torch.sigmoid(slope * delta)
                    weighted_mse = ((img_pred_matched - latent_target) ** 2) * weight
                    loss = weighted_mse.mean() / gradient_accumulation_steps
                    # å…œåº•ï¼šå¦‚å‡ºç°éæœ‰é™å€¼ï¼Œå›é€€åˆ°çº¯ MSE
                    if not torch.isfinite(loss):
                        loss = F.mse_loss(img_pred_matched, latent_target, reduction='mean')
                        loss = loss / gradient_accumulation_steps
                    
                    # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦ä¸ºNaN
                    if torch.isnan(loss):
                        print("âŒ æŸå¤±å€¼ä¸ºNaNï¼Œè·³è¿‡æ­¤æ­¥")
                        continue
                    
                    if global_step % 100 == 0:
                        print(f"âœ… æŸå¤±è®¡ç®—æ­£å¸¸: {loss.item():.6f}")

                if global_step % 100 == 0:
                    print(f"ğŸ‰ å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—æˆåŠŸï¼Loss: {loss.item():.6f}")
                    print("å¼€å§‹åå‘ä¼ æ’­...")
                
                # åå‘ä¼ æ’­
                scaler.scale(loss).backward()

                # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                try:
                    scaler.unscale_(optimizer)
                    max_norm = 0.5 if ((not self.is_stage1) and (global_step < self.stage1_steps + 300)) else 1.0
                    torch.nn.utils.clip_grad_norm_(
                        list(model.parameters()) + list(proj_hidden.parameters()) + list(proj_pooled.parameters()),
                        max_norm=max_norm
                    )
                except Exception:
                    pass

                # åªåœ¨ç´¯ç§¯æ­¥æ•°è¾¾åˆ°æ—¶æ‰æ›´æ–°ä¼˜åŒ–å™¨
                if (global_step + 1) % gradient_accumulation_steps == 0:
                    scaler.step(optimizer)
                    scaler.update()
                    # é¢å¤–ç¨³å¥åŒ–ï¼šé™åˆ¶ x_embedder æƒé‡èŒƒæ•°ï¼Œé˜²æ­¢çˆ†ç‚¸
                    try:
                        if hasattr(model, 'x_embedder'):
                            with torch.no_grad():
                                w = model.x_embedder.weight
                                norm = w.norm()
                                max_norm = 60.0
                                if torch.isfinite(norm) and norm.item() > max_norm:
                                    w.mul_(max_norm / (norm + 1e-6))
                    except Exception:
                        pass
                    optimizer.zero_grad()
                    torch.cuda.empty_cache()

                if global_step % 10 == 0:
                    print(f"Epoch {epoch} Step {global_step} Loss: {loss.item() * gradient_accumulation_steps:.4f}")
                if global_step % 100 == 0:
                    try:
                        print(f"x_embedder.weight.norm(): {model.x_embedder.weight.norm().item():.6f}")
                        print(f"proj_hidden.weight.norm(): {proj_hidden.weight.norm().item():.6f}")
                        print(f"å½“å‰å­¦ä¹ ç‡: {[g['lr'] for g in optimizer.param_groups]}")
                    except Exception:
                        pass
                
                # é‡‡æ ·åŠŸèƒ½ï¼ˆåŒ…æ‹¬ step=0ï¼‰ï¼Œè¾“å‡ºåˆ°å¸¦æ—¶é—´æˆ³çš„ training_folder/samples
                if global_step % 1000 == 0:
                    self.sample_images(model, vae, text_encoder, text_encoder_2, 
                                     clip_tokenizer, t5_tokenizer, proj_hidden, proj_pooled, 
                                     global_step, device)
                    
                # æ¨¡å‹ä¿å­˜ï¼šè¾“å‡ºåˆ°å¸¦æ—¶é—´æˆ³çš„ training_folder/checkpoints
                if global_step % 2000 == 0 and global_step > 0:
                    self.save_model(model, vae, text_encoder, text_encoder_2, 
                                  clip_tokenizer, t5_tokenizer, proj_hidden, proj_pooled, 
                                  global_step, device)
                    
                global_step += 1
                if global_step >= total_steps:  # è®­ç»ƒåˆ°æŒ‡å®šæ­¥æ•°
                    print(f"âœ… è®­ç»ƒå®Œæˆï¼å…±è®­ç»ƒ {total_steps} æ­¥")
                    break
            epoch += 1
            # while å¾ªç¯ä¼šæ ¹æ® global_step å†³å®šæ˜¯å¦ç»§ç»­
                    
        print("è®­ç»ƒå®Œæˆ")
    
    def _freeze_all_except_projection(self, model):
        """å†»ç»“é™¤æŠ•å½±å±‚å¤–çš„æ‰€æœ‰å‚æ•°"""
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = False
        
        # åªè§£å†» x_embedder
        for param in model.x_embedder.parameters():
            param.requires_grad = True
            
        print(f"â„ï¸  ç¬¬ä¸€é˜¶æ®µï¼šå†»ç»“å…¨æ¨¡å‹ï¼Œåªè®­ç»ƒ x_embedder")
    
    def _unfreeze_all_parameters(self, model):
        """è§£å†»æ‰€æœ‰å‚æ•°ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰"""
        # è§£å†»æ‰€æœ‰å‚æ•°
        for param in model.parameters():
            param.requires_grad = True
            
        print(f"ğŸ”¥ ç¬¬äºŒé˜¶æ®µï¼šè§£å†»å…¨æ¨¡å‹å‚æ•°")

    def _unfreeze_last_n_blocks(self, model, n: int = 4):
        """ä»…è§£å†»æœ€å n ä¸ª block ä¸ x_embedderï¼Œé™ä½é˜¶æ®µ2æ˜¾å­˜å‹åŠ›"""
        # å…ˆå…¨éƒ¨å†»ç»“
        for p in model.parameters():
            p.requires_grad = False
        # å§‹ç»ˆè§£å†» x_embedder
        if hasattr(model, 'x_embedder'):
            for p in model.x_embedder.parameters():
                p.requires_grad = True
        # å°è¯•æ‰¾åˆ° blocks åˆ—è¡¨å±æ€§
        blocks_attr = None
        for attr in ['transformer_blocks', 'layers', 'blocks']:
            if hasattr(model, attr):
                blocks_attr = attr
                break
        if blocks_attr is not None:
            blocks = getattr(model, blocks_attr)
            try:
                total = len(blocks)
                start = max(0, total - n)
                for i in range(start, total):
                    for p in blocks[i].parameters():
                        p.requires_grad = True
                print(f"ğŸ”¥ ç¬¬äºŒé˜¶æ®µï¼šä»…è§£å†»æœ€å {n} ä¸ªblockï¼ˆattr={blocks_attr}ï¼Œå…±{total}ä¸ªï¼‰")
                return True
            except Exception:
                pass
        # å›é€€ï¼šè§£å†»å…¨éƒ¨
        for p in model.parameters():
            p.requires_grad = True
        print("âš ï¸ æœªæ‰¾åˆ°å¯è¯†åˆ«çš„blockså±æ€§ï¼Œå·²å›é€€ä¸ºè§£å†»å…¨æ¨¡å‹")
        return False
    
    def update_training_stage(self, current_step: int, model, optimizer):
        """
        æ›´æ–°è®­ç»ƒé˜¶æ®µ
        Args:
            current_step: å½“å‰è®­ç»ƒæ­¥æ•°
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
        """
        self.current_step = current_step
        
        if self.two_stage_training:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µ
            if self.is_stage1 and current_step >= self.stage1_steps:
                print(f"ğŸ”„ åˆ‡æ¢åˆ°ç¬¬äºŒé˜¶æ®µè®­ç»ƒ (æ­¥æ•°: {current_step})")
                # ä»…è§£å†»æœ€å4ä¸ª block ä»¥é™ä½é˜¶æ®µ2æ˜¾å­˜
                ok = self._unfreeze_last_n_blocks(model, n=4)
                if not ok:
                    self._unfreeze_all_parameters(model)
                self.is_stage1 = False
                
                # æ›´æ–°å­¦ä¹ ç‡
                for param_group in optimizer.param_groups:
                    param_group['lr'] = self.stage2_lr
                print(f"ğŸ“ˆ å­¦ä¹ ç‡æ›´æ–°ä¸º: {self.stage2_lr}")
                
                return True  # è¿”å›Trueè¡¨ç¤ºé˜¶æ®µåˆ‡æ¢
        
        return False
    
    def sample_images(self, model, vae, text_encoder, text_encoder_2, 
                     clip_tokenizer, t5_tokenizer, proj_hidden, proj_pooled, 
                     step, device):
        """é‡‡æ ·ç”Ÿæˆå›¾åƒ"""
        print(f"ğŸ¨ å¼€å§‹é‡‡æ · (æ­¥æ•°: {step})")
        
        # é‡‡æ ·é…ç½®
        prompts = [
            "add a sofa",
            "add a table", 
            "add a bed",
            "add furniture"
        ]
        
        # ä½¿ç”¨å›ºå®šçš„æµ‹è¯•å›¾åƒ
        test_image_path = "/cloud/cloud-ssd1/test.png"
        if not os.path.exists(test_image_path):
            print(f"âš ï¸ æµ‹è¯•å›¾åƒä¸å­˜åœ¨: {test_image_path}")
            return
            
        try:
            from PIL import Image
            import torchvision.transforms as transforms
            
            # åŠ è½½æµ‹è¯•å›¾åƒ
            test_image = Image.open(test_image_path).convert('RGB')
            test_image = test_image.resize((512, 512))
            transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize([0.5], [0.5])
            ])
            test_tensor = transform(test_image).unsqueeze(0).to(device)
            
            # åˆ›å»ºçº¯ç™½æ§åˆ¶å›¾åƒ
            white_image = torch.ones_like(test_tensor)
            
            # VAEç¼–ç 
            with torch.no_grad():
                if torch.cuda.device_count() > 1:
                    test_latent = vae.encode(test_tensor.to('cuda:1')).latent_dist.sample().to('cuda:0')
                    white_latent = vae.encode(white_image.to('cuda:1')).latent_dist.sample().to('cuda:0')
                else:
                    test_latent = vae.encode(test_tensor).latent_dist.sample()
                    white_latent = vae.encode(white_image).latent_dist.sample()
            
            # ç”Ÿæˆ4å¼ å›¾åƒ
            for i, prompt in enumerate(prompts):
                # æ–‡æœ¬ç¼–ç ï¼šç»Ÿä¸€åœ¨ CPUï¼Œä¸ŠæŠ•å½±å‰å†æ¬åˆ° GPU
                with torch.no_grad():
                    text_device = 'cpu'
                    clip_ids = clip_tokenizer(
                        prompt, padding='max_length',
                        max_length=clip_tokenizer.model_max_length,
                        truncation=True, return_tensors='pt'
                    ).input_ids.to(text_device)
                    clip_out = text_encoder(
                        clip_ids, output_hidden_states=True, return_dict=True
                    )
                    hidden_states = clip_out.last_hidden_state  # CPU
                    pooled_state = clip_out.text_embeds          # CPU
                    
                    t5_ids = t5_tokenizer(
                        prompt, padding='max_length',
                        max_length=t5_tokenizer.model_max_length,
                        truncation=True, return_tensors='pt'
                    ).input_ids.to(text_device)
                    t5_out = text_encoder_2(
                        t5_ids, output_hidden_states=True, return_dict=True
                    )
                    t5_hidden_states = t5_out.last_hidden_state

                # æŠ•å½±ï¼ˆGPUï¼‰
                enc_states = proj_hidden(hidden_states.to(device))
                pool_proj = proj_pooled(pooled_state.to(device))
                
                # å‡†å¤‡æ¨¡å‹è¾“å…¥
                test_latent_16 = test_latent[:, :16, :, :]
                white_latent_16 = white_latent[:, :16, :, :]
                
                # æ‹¼æ¥ä¸è®­ç»ƒå¯¹é½ï¼šåŸå›¾16 + æ§åˆ¶å›¾16 = 32é€šé“
                model_input = torch.cat([test_latent_16, white_latent_16], dim=1)
                
                # è¡¥é›¶åˆ°64é€šé“
                if model_input.shape[1] < 64:
                    pad = torch.zeros((model_input.shape[0], 64 - model_input.shape[1], 
                                     model_input.shape[2], model_input.shape[3]), 
                                    device=model_input.device, dtype=model_input.dtype)
                    model_input = torch.cat([model_input, pad], dim=1)
                
                # æ¨¡å‹å‰å‘
                B, C, H, W = model_input.shape
                model_input_permuted = model_input.permute(0, 2, 3, 1).contiguous()
                model_input_seq = model_input_permuted.reshape(model_input_permuted.shape[0], -1, model_input_permuted.shape[-1])
                
                # æ„é€ ä½ç½®ç¼–ç 
                txt_ids = torch.zeros(B, enc_states.shape[1], 2, device=device, dtype=torch.long)
                for b in range(B):
                    for j in range(enc_states.shape[1]):
                        txt_ids[b, j] = torch.tensor([j // W, j % W], device=device)
                
                y_coords = torch.arange(H, device=device).unsqueeze(1).repeat(1, W).flatten()
                x_coords = torch.arange(W, device=device).unsqueeze(0).repeat(H, 1).flatten()
                img_ids_single = torch.stack([y_coords, x_coords], dim=1)
                img_ids = img_ids_single.unsqueeze(0).expand(B, -1, -1)
                
                timesteps = torch.zeros(B, dtype=torch.float32, device=device)
                guidance = timesteps.clone()
                
                with torch.no_grad():
                    pred = model(
                        hidden_states=model_input_seq,
                        encoder_hidden_states=enc_states,
                        pooled_projections=pool_proj,
                        guidance=guidance,
                        timestep=timesteps,
                        txt_ids=txt_ids,
                        img_ids=img_ids,
                        return_dict=False
                    )[0]
                
                # å¤„ç†è¾“å‡º
                img_pred = pred
                B, img_seq_len, out_channels = img_pred.shape
                H = W = int(img_seq_len ** 0.5)
                img_pred_reshaped = img_pred.permute(0, 2, 1).reshape(B, out_channels, H, W)
                img_pred_matched = img_pred_reshaped[:, :16, :, :]
                
                # VAEè§£ç 
                with torch.no_grad():
                    if torch.cuda.device_count() > 1:
                        decoded = vae.decode(img_pred_matched.to('cuda:1')).sample.to('cuda:0')
                    else:
                        decoded = vae.decode(img_pred_matched).sample
                
                # ä¿å­˜å›¾åƒ
                import torchvision.utils as vutils
                # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„è®­ç»ƒæ ¹ç›®å½•
                output_root = getattr(self, 'training_folder', None)
                if output_root is None:
                    output_root = getattr(self.job, 'training_folder', "/cloud/cloud-ssd1/training_output")
                output_dir = os.path.join(output_root, "samples")
                os.makedirs(output_dir, exist_ok=True)
                
                # åˆ›å»ºå¯¹æ¯”å›¾ï¼šåŸå›¾ + ç»“æœ
                comparison = torch.cat([test_tensor, decoded], dim=3)  # æ°´å¹³æ‹¼æ¥
                
                # æ·»åŠ æ–‡æœ¬åˆ°å›¾åƒ
                from PIL import Image, ImageDraw, ImageFont
                comparison_pil = vutils.make_grid(comparison, nrow=1, padding=0, normalize=True)
                comparison_pil = transforms.ToPILImage()(comparison_pil)
                
                # æ·»åŠ æ–‡æœ¬
                draw = ImageDraw.Draw(comparison_pil)
                try:
                    font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 20)
                except:
                    font = ImageFont.load_default()
                
                draw.text((10, 10), f"Step {step} - {prompt}", fill="white", font=font)
                
                # ä¿å­˜
                output_path = f"{output_dir}/step_{step:06d}_sample_{i:02d}.jpg"
                comparison_pil.save(output_path)
                print(f"ğŸ’¾ ä¿å­˜é‡‡æ ·å›¾åƒ: {output_path}")
                
        except Exception as e:
            print(f"âŒ é‡‡æ ·å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def save_model(self, model, vae, text_encoder, text_encoder_2, 
                  clip_tokenizer, t5_tokenizer, proj_hidden, proj_pooled, 
                  step, device):
        """ä¿å­˜æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹ (æ­¥æ•°: {step})")
        
        try:
            # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„è®­ç»ƒæ ¹ç›®å½•
            output_root = getattr(self, 'training_folder', None)
            if output_root is None:
                output_root = getattr(self.job, 'training_folder', "/cloud/cloud-ssd1/training_output")
            output_dir = os.path.join(output_root, f"checkpoints/step_{step:06d}")
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜transformer
            model.save_pretrained(f"{output_dir}/transformer")
            
            # ä¿å­˜VAE
            vae.save_pretrained(f"{output_dir}/vae")
            
            # ä¿å­˜æ–‡æœ¬ç¼–ç å™¨
            text_encoder.save_pretrained(f"{output_dir}/text_encoder")
            text_encoder_2.save_pretrained(f"{output_dir}/text_encoder_2")
            
            # ä¿å­˜tokenizer
            clip_tokenizer.save_pretrained(f"{output_dir}/tokenizer")
            t5_tokenizer.save_pretrained(f"{output_dir}/tokenizer_2")
            
            # ä¿å­˜æŠ•å½±å±‚
            torch.save(proj_hidden.state_dict(), f"{output_dir}/proj_hidden.pt")
            torch.save(proj_pooled.state_dict(), f"{output_dir}/proj_pooled.pt")
            
            # ä¿å­˜è®­ç»ƒä¿¡æ¯
            import yaml
            meta = {
                'step': step,
                'two_stage_training': self.two_stage_training,
                'stage1_steps': self.stage1_steps,
                'stage1_lr': self.stage1_lr,
                'stage2_lr': self.stage2_lr,
                'is_stage1': self.is_stage1
            }
            with open(f"{output_dir}/meta.yaml", 'w') as f:
                yaml.dump(meta, f)
            
            print(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆ: {output_dir}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()