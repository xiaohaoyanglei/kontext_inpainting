#!/usr/bin/env python3
"""
æµ‹è¯•æ ‡å‡† sd_trainer æ˜¯å¦èƒ½æ­£ç¡®å¤„ç† inpainting æ•°æ®

ä½¿ç”¨æ­¤è„šæœ¬æµ‹è¯•è¿ç§»åçš„é…ç½®æ–‡ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import yaml
import torch
import os
from pathlib import Path
import argparse

def test_config_loading(config_path):
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("ğŸ“‹ æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½...")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®é…ç½®é¡¹
        process_config = config['config']['process'][0]
        
        print(f"   è®­ç»ƒå™¨ç±»å‹: {process_config['type']}")
        print(f"   æ¨¡å‹è·¯å¾„: {process_config['model']['name_or_path']}")
        print(f"   æ˜¯å¦å¯ç”¨FLUX: {process_config['model']['is_flux']}")
        
        # æ£€æŸ¥æ•°æ®é›†é…ç½®
        dataset_config = process_config['datasets'][0]
        print(f"   ç›®æ ‡å›¾åƒè·¯å¾„: {dataset_config['folder_path']}")
        print(f"   å¸¦æ´å›¾åƒè·¯å¾„: {dataset_config.get('inpaint_path', 'None')}")
        print(f"   æ©ç è·¯å¾„: {dataset_config.get('mask_path', 'None')}")
        print(f"   ç¼“å­˜latents: {dataset_config.get('cache_latents_to_disk', False)}")
        
        return config
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None

def test_data_paths(config):
    """æµ‹è¯•æ•°æ®è·¯å¾„æ˜¯å¦å­˜åœ¨"""
    print("\nğŸ“ æµ‹è¯•æ•°æ®è·¯å¾„...")
    
    dataset_config = config['config']['process'][0]['datasets'][0]
    
    paths_to_check = [
        ("ç›®æ ‡å›¾åƒ", dataset_config['folder_path']),
        ("å¸¦æ´å›¾åƒ", dataset_config.get('inpaint_path')),
        ("æ©ç ", dataset_config.get('mask_path'))
    ]
    
    all_exist = True
    
    for name, path in paths_to_check:
        if path and Path(path).exists():
            file_count = len(list(Path(path).glob('*.*')))
            print(f"âœ… {name}è·¯å¾„å­˜åœ¨: {path} ({file_count} ä¸ªæ–‡ä»¶)")
        else:
            print(f"âŒ {name}è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸ºç©º: {path}")
            all_exist = False
    
    return all_exist

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print("\nğŸ–¥ï¸  æµ‹è¯•GPUå¯ç”¨æ€§...")
    
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        
        return True
    else:
        print("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨GPU")
        return False

def test_dataloader_creation(config_path):
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»º"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½å™¨åˆ›å»º...")
    
    try:
        # è¿™é‡Œæˆ‘ä»¬åªæ˜¯å¯¼å…¥å¿…è¦çš„æ¨¡å—æ¥æµ‹è¯•
        from toolkit.config_modules import DatasetConfig, preprocess_dataset_raw_config
        from toolkit.data_loader import get_dataloader_from_datasets
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        dataset_raw_config = config['config']['process'][0]['datasets'][0]
        
        # é¢„å¤„ç†é…ç½®
        split_configs = preprocess_dataset_raw_config([dataset_raw_config])
        dataset_config = DatasetConfig(**split_configs[0])
        
        print(f"âœ… æ•°æ®é›†é…ç½®åˆ›å»ºæˆåŠŸ")
        print(f"   æ–‡ä»¶å¤¹è·¯å¾„: {dataset_config.folder_path}")
        print(f"   Inpaintè·¯å¾„: {dataset_config.inpaint_path}")
        print(f"   Maskè·¯å¾„: {dataset_config.mask_path}")
        print(f"   ç¼“å­˜latents: {dataset_config.cache_latents_to_disk}")
        print(f"   åˆ†è¾¨ç‡: {dataset_config.resolution}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½èƒ½åŠ›"""
    print("\nğŸ¤– æµ‹è¯•æ¨¡å‹è®¿é—®...")
    
    try:
        # å°è¯•å¯¼å…¥huggingface_hub
        try:
            from huggingface_hub import list_repo_files
        except ImportError:
            print("âš ï¸  huggingface_hubæœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹è®¿é—®æµ‹è¯•")
            return True
        
        # æ£€æŸ¥FLUXæ¨¡å‹æ˜¯å¦å¯è®¿é—®
        repo_id = "black-forest-labs/FLUX.1-dev"
        files = list_repo_files(repo_id)
        
        key_files = ['model_index.json', 'scheduler/scheduler_config.json']
        found_files = [f for f in files if any(key in f for key in key_files)]
        
        print(f"âœ… FLUX.1-devæ¨¡å‹å¯è®¿é—®")
        print(f"   æ‰¾åˆ°å…³é”®æ–‡ä»¶: {len(found_files)} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹è®¿é—®æµ‹è¯•å¤±è´¥: {e}")
        print("   (è¿™å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–éœ€è¦HuggingFaceä»¤ç‰Œ)")
        return False

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•æ ‡å‡†sd_traineré…ç½®")
    parser.add_argument("config_path", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--skip_dataloader", action="store_true", help="è·³è¿‡æ•°æ®åŠ è½½å™¨æµ‹è¯•")
    parser.add_argument("--skip_model", action="store_true", help="è·³è¿‡æ¨¡å‹è®¿é—®æµ‹è¯•")
    
    args = parser.parse_args()
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æ ‡å‡†traineré…ç½®...")
    print("=" * 50)
    
    # æµ‹è¯•é…ç½®æ–‡ä»¶
    config = test_config_loading(args.config_path)
    if not config:
        return
    
    # æµ‹è¯•æ•°æ®è·¯å¾„
    data_paths_ok = test_data_paths(config)
    
    # æµ‹è¯•GPU
    gpu_ok = test_gpu_availability()
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    if not args.skip_dataloader:
        dataloader_ok = test_dataloader_creation(args.config_path)
    else:
        dataloader_ok = True
        print("\nğŸ“Š è·³è¿‡æ•°æ®åŠ è½½å™¨æµ‹è¯•")
    
    # æµ‹è¯•æ¨¡å‹è®¿é—®
    if not args.skip_model:
        model_ok = test_model_loading()
    else:
        model_ok = True
        print("\nğŸ¤– è·³è¿‡æ¨¡å‹è®¿é—®æµ‹è¯•")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    
    tests = [
        ("é…ç½®æ–‡ä»¶", config is not None),
        ("æ•°æ®è·¯å¾„", data_paths_ok),
        ("GPUå¯ç”¨æ€§", gpu_ok),
        ("æ•°æ®åŠ è½½å™¨", dataloader_ok),
        ("æ¨¡å‹è®¿é—®", model_ok)
    ]
    
    passed = sum(1 for _, result in tests if result)
    total = len(tests)
    
    for name, result in tests:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {name}: {status}")
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸš€ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒï¼š")
        print(f"   python run.py {args.config_path}")
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        
        if not data_paths_ok:
            print("\nğŸ’¡ æ•°æ®è·¯å¾„é—®é¢˜è§£å†³æ–¹æ¡ˆï¼š")
            print("   1. æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print("   2. è¿è¡Œæ•°æ®è¿ç§»è„šæœ¬ï¼š")
            print("      python scripts/migrate_inpainting_data.py --source_dir ... --target_dir ... --mask_dir ... --output_dir ...")


if __name__ == "__main__":
    main() 